{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1 + Task2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of this competition: \n",
    "\n",
    "- predict the category of each news. \n",
    "\n",
    "\n",
    "Evaluation: \n",
    "\n",
    "- Avg value of f1_score. \n",
    "\n",
    "- A comparation between submitting category result and real category data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12281897.0.0.7a212b4amWToWd&postId=95703\n",
    "\n",
    "Task1 赛题理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.9.6406111aIKCSLV&postId=118253\n",
    "\n",
    "Task2 数据读取与数据分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12281897.0.0.7a212b4amWToWd&postId=118254\n",
    "\n",
    "https://github.com/datawhalechina/team-learning-nlp/blob/master/NewsTextClassification/Task3%20%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.md\n",
    "\n",
    "Task3 基于机器学习的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12281897.0.0.7a212b4amWToWd&postId=118255\n",
    "\n",
    "Task4 基于深度学习的文本分类1-fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tianchi.aliyun.com/notebook-ai/?spm=5176.12282024.0.0.432d14ca2Inpak\n",
    "\n",
    "https://github.com/datawhalechina/team-learning-nlp/tree/master/NewsTextClassification\n",
    "    \n",
    "All Task1-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://shimo.im/docs/rp3OVnXzDWtpoxAm/read\n",
    "    \n",
    "石墨文档大纲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**听课笔记**\n",
    "- Part1 比赛介绍\n",
    "- Part2 baseline\n",
    "- Part3 比赛知识点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "一个进行文本分类的数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么匿名数据集？\n",
    "- 没有版权问题；\n",
    "- 需要从头构建词向量。（不能分词）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思路：（四种方法，由易到难）\n",
    "- TF-IDF + RidgeClassifier ; (F1_score ~ 0.93)\n",
    "- FastText ; (F1_score ~ 0.93-0.94)\n",
    "- Word2Vec + TextCNN ; (F1_score ~ 0.95-0.97)\n",
    "- Bert.(F1_score ~ 0.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思路1 \n",
    "1 ：将字符进行 TF-IDF (term frequency–inverse document frequency）统计，然后送入线性分类器进行训练；\n",
    "- CountVectorizer RidgeClassifier\n",
    "- TfidfVectorizer RidgeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。tf-idf加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了tf-idf以外，互联网上的搜索引擎还会使用基于链接分析的评级方法，以确定文件在搜索结果中出现的顺序。)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline\n",
    "- 如何继续深入，提高精度？\n",
    "- 尝试其他机器学习模型；\n",
    "- 对 TF IDF 和 ngram 进行 gridsearch\n",
    "- 尝试思路 2 、思路 3 、思路 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其它尝试和思考：\n",
    "你能分析得出匿名字符中的标点符号吗？ （可以出现结尾，但不开头）\n",
    "你知道 NLP 中哪些数据扩增方法呢？（同义词替换，添加某个单词，等等）\n",
    "线上 F1 打分能到 0.99 吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding and exploratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testa=pd.read_csv('C:/Chen/1_Coding/20200721_nlp_learning/test_a.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('C:/Chen/1_Coding/20200721_nlp_learning/train_set.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5399 3117 1070 4321 4568 2621 5466 3772 4516 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2491 4109 1757 7539 648 3695 3038 4490 23 7019...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2673 5076 6835 2835 5948 5677 3247 4124 2465 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4562 4893 2210 4761 3659 1324 2595 5949 4583 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4269 7134 2614 1724 4464 1324 3370 3370 2106 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>3725 4498 2282 1647 6293 4245 4498 3615 1141 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>4811 465 3800 1394 3038 2376 2327 5165 3070 57...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>5338 1952 3117 4109 299 6656 6654 3792 6831 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>893 3469 5775 584 2490 4223 6569 6663 2124 168...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>2400 4409 4412 2210 5122 4464 7186 2465 1327 9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      5399 3117 1070 4321 4568 2621 5466 3772 4516 2...\n",
       "1      2491 4109 1757 7539 648 3695 3038 4490 23 7019...\n",
       "2      2673 5076 6835 2835 5948 5677 3247 4124 2465 5...\n",
       "3      4562 4893 2210 4761 3659 1324 2595 5949 4583 2...\n",
       "4      4269 7134 2614 1724 4464 1324 3370 3370 2106 2...\n",
       "49995  3725 4498 2282 1647 6293 4245 4498 3615 1141 2...\n",
       "49996  4811 465 3800 1394 3038 2376 2327 5165 3070 57...\n",
       "49997  5338 1952 3117 4109 299 6656 6654 3792 6831 21...\n",
       "49998  893 3469 5775 584 2490 4223 6569 6663 2124 168...\n",
       "49999  2400 4409 4412 2210 5122 4464 7186 2465 1327 9..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testa.head().append(df_testa.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head().append(df_train.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testa.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testa.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to know the length of sentences\n",
    "#The number of words in sentence\n",
    "df_train['text_len'] = df_train['text'].apply(lambda x: len(x.split(' ')))\n",
    "print(df_test['text_len'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In resume, this competition gives relatively long sentences. A sentence is composed by 907 words in average, with the minimum length of 2, and the maximum length of 44665 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viz of sentences' length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viz of sentences' length\n",
    "_ = plt.hist(df_train['text_len'], bins=200)\n",
    "plt.xlabel('Text char count')\n",
    "plt.title(\"Histogram of char count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viz of news category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viz of news category\n",
    "df_train['label'].value_counts().plot(kind='bar')\n",
    "plt.title('News class count')\n",
    "plt.xlabel(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在数据集中标签的对应的关系如下：{'科技': 0, '股票': 1, '体育': 2, '娱乐': 3, '时政': 4, '社会': 5, '教育': 6, '财经': 7, '家居': 8, '游戏': 9, '房产': 10, '时尚': 11, '彩票': 12, '星座': 13}\n",
    "从统计结果可以看出，赛题的数据集类别分布存在较为不均匀的情况。在训练集中科技类新闻最多，其次是股票类新闻，最少的新闻是星座新闻。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequencty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the most/least frequent word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines = ' '.join(list(df_train['text']))\n",
    "type(all_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter(all_lines.split(\" \"))\n",
    "word_count = sorted(word_count.items(), key=lambda d:int(d[1]), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_count))\n",
    "# in total there are 6869 words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_count[0])\n",
    "print(word_count[1])\n",
    "print(word_count[2])\n",
    "print(word_count[3])\n",
    "print(word_count[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_count[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find punctuation marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里还可以根据字在每个句子的出现情况，反推出标点符号。下面代码统计了不同字符在句子中出现的次数，其中字符3750，字符900和字符648在20w新闻的覆盖率接近99%，很有可能是标点符号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text_unique'] = df_train['text'].apply(lambda x: ' ' .join(list(set(x.split(' ')))))\n",
    "#df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines1 = ' '.join(list(df_train['text_unique']))\n",
    "word_count1 = Counter(all_lines1.split(\" \"))\n",
    "word_count1 = sorted(word_count1.items(), key=lambda d:int(d[1]), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_count1[0])\n",
    "print(word_count1[1])\n",
    "print(word_count1[2])\n",
    "print(word_count1[3])\n",
    "print(word_count1[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据分析的结论\n",
    "通过上述分析我们可以得出以下结论：\n",
    "\n",
    "赛题中每个新闻包含的字符个数平均为1000个，还有一些新闻字符较长；\n",
    "赛题中新闻类别分布不均匀，科技类新闻样本量接近4w，星座类新闻样本量不到1k；\n",
    "赛题总共包括7000-8000个字符；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过数据分析，我们还可以得出以下结论：\n",
    "\n",
    "每个新闻平均字符个数较多，可能需要截断；\n",
    "\n",
    "由于类别不均衡，会严重影响模型的精度；\n",
    "\n",
    "本章小结\n",
    "本章对赛题数据进行读取，并新闻句子长度、类别和字符进行了可视化分析。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章作业\n",
    "假设字符3750，字符900和字符648是句子的标点符号，请分析赛题每篇新闻平均由多少个句子构成？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(word_count[0][0])\n",
    "#word_count[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_number = int(word_count[0][0]) + int(word_count[1][0]) + int(word_count[2][0])\n",
    "sentence_number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "统计每类新闻中出现次数对多的字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_train.groupby(['label'], sort=True)['text_len'].max()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task3 基于机器学习的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一章节，我们对赛题的数据进行了读取，并在末尾给出了两个小作业。如果你顺利完成了作业，那么你基本上对`Python`也比较熟悉了。在本章我们将使用传统机器学习算法来完成新闻分类的过程，将会结束到赛题的核心知识点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于机器学习的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本章我们将开始使用机器学习模型来解决文本分类。机器学习发展比较广，且包括多个分支，本章侧重使用传统机器学习，从下一章开始是基于深度学习的文本分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 学会TF-IDF的原理和使用\n",
    "- 使用sklearn的机器学习模型完成文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 机器学习模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "机器学习是对能通过经验自动改进的计算机算法的研究。机器学习通过历史数据**训练**出**模型**对应于人类对经验进行**归纳**的过程，机器学习利用**模型**对新数据进行**预测**对应于人类利用总结的**规律**对新问题进行**预测**的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习有很多种分支，对于学习者来说应该优先掌握机器学习算法的分类，然后再其中一种机器学习算法进行学习。由于机器学习算法的分支和细节实在是太多，所以如果你一开始就被细节迷住了眼，你就很难知道全局是什么情况的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你是机器学习初学者，你应该知道如下的事情：\n",
    "\n",
    "1. 机器学习能解决一定的问题，但不能奢求机器学习是万能的；\n",
    "2. 机器学习算法有很多种，看具体问题需要什么，再来进行选择；\n",
    "3. 每种机器学习算法有一定的偏好，需要具体问题具体分析；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![machine_learning_overview](https://img-blog.csdnimg.cn/20200714203223253.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本表示方法 Part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在机器学习算法的训练过程中，假设给定$N$个样本，每个样本有$M$个特征，这样组成了$N×M$的样本矩阵，然后完成算法的训练和预测。同样的在计算机视觉中可以将图片的像素看作特征，每张图片看作hight×width×3的特征图，一个三维的矩阵来进入计算机进行计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是在自然语言领域，上述方法却不可行：文本是不定长度的。文本表示成计算机能够运算的数字或向量的方法一般称为词嵌入（Word Embedding）方法。词嵌入将不定长的文本转换到定长的空间内，是文本分类的第一步。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的One-hot与数据挖掘任务中的操作是一致的，即将每一个单词使用一个离散的向量表示。具体将每个字/词编码一个索引，然后根据索引进行赋值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot表示方法的例子如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "句子1：我 爱 北 京 天 安 门\n",
    "句子2：我 喜 欢 上 海\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先对所有句子的字进行索引，即将每个字确定一个编号："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "{\n",
    "\t'我': 1, '爱': 2, '北': 3, '京': 4, '天': 5,\n",
    "  '安': 6, '门': 7, '喜': 8, '欢': 9, '上': 10, '海': 11\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里共包括11个字，因此每个字可以转换为一个11维度稀疏向量："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "我：[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "爱：[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "...\n",
    "海：[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words（词袋表示），也称为Count Vectors，每个文档的字/词可以使用其出现次数来进行表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "句子1：我 爱 北 京 天 安 门\n",
    "句子2：我 喜 欢 上 海\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接统计每个字出现的次数，并进行赋值："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "句子1：我 爱 北 京 天 安 门\n",
    "转换为 [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "\n",
    "句子2：我 喜 欢 上 海\n",
    "转换为 [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在sklearn中可以直接`CountVectorizer`来实现这一步骤："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer() \n",
    "vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "# CountVectorizer() : Get the term frequency\n",
    "# n_gram好像是用来组词的，几个词一组？\n",
    "#大小写不敏感\n",
    "#出现的英文词，按alphabet重排序了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram与Count Vectors类似，不过加入了相邻单词组合成为新的单词，并进行计数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果N取值为2，则句子1和句子2就变为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "句子1：我爱 爱北 北京 京天 天安 安门\n",
    "句子2：我喜 喜欢 欢上 上海\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF 分数由两部分组成：第一部分是**词语频率**（Term Frequency），第二部分是**逆文档频率**（Inverse Document Frequency）。其中计算语料库中文档总数除以含有该词语的文档数量，然后再取对数就是逆文档频率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TF(t)= 该词语在当前文档出现的次数 / 当前文档中词语的总数\n",
    "IDF(t)= log_e（文档总数 / 出现该词语的文档总数）\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于机器学习的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们将对比不同文本表示算法的精度，通过本地构建验证集计算F1得分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectors + RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import f1_score \n",
    "\n",
    "\n",
    "df_train=pd.read_csv('C:/Chen/1_Coding/20200721_nlp_learning/train_set.csv',sep='\\t', nrows=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65441877581244\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=3000)\n",
    "train_test = vectorizer.fit_transform(df_train['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:10000], df_train['label'].values[:10000])\n",
    "\n",
    "val_pred = clf.predict(train_test[10000:])\n",
    "print(f1_score(df_train['label'].values[10000:], val_pred, average='macro'))\n",
    "#0.654430246247168"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  TF-IDF +  RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "df_train=pd.read_csv('C:/Chen/1_Coding/20200721_nlp_learning/train_set.csv',sep='\\t', nrows=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8717702514414455\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=3000)\n",
    "train_test = tfidf.fit_transform(df_train['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:10000], df_train['label'].values[:10000])\n",
    "\n",
    "val_pred = clf.predict(train_test[10000:])\n",
    "print(f1_score(df_train['label'].values[10000:], val_pred, average='macro'))\n",
    "\n",
    "# CountVectorizer() : Get the term frequency\n",
    "# TfidfVectorizer() : based on CountVectorizer() \n",
    "# 需要先用CountVectorizer()\n",
    "#0.8719372173702"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**以下是一个来自Sklearn官方文档的例子**\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "\n",
    "另外还有两个相关文档：\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\narray([[1, 1, 1, 1, 0, 1, 0, 0],\\n       [1, 2, 0, 1, 1, 1, 0, 0],\\n       [1, 0, 0, 1, 0, 1, 1, 1],\\n       [1, 1, 1, 1, 0, 1, 0, 0]])\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "corpus = ['this is the first document',\n",
    "          'this document is the second document',\n",
    "          'and this is the third one',\n",
    "          'is this the first document']\n",
    "vocabulary = ['this', 'document', 'first', 'is', 'second', 'the', 'and', 'one']\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)), \n",
    "                 # CountVectorizer() : 计算文档中所有包括该词的数目\n",
    "                 ('tfid', TfidfTransformer())]).fit(corpus)\n",
    "pipe['count'].transform(corpus).toarray()\n",
    "\"\"\"\n",
    "array([[1, 1, 1, 1, 0, 1, 0, 0],\n",
    "       [1, 2, 0, 1, 1, 1, 0, 0],\n",
    "       [1, 0, 0, 1, 0, 1, 1, 1],\n",
    "       [1, 1, 1, 1, 0, 1, 0, 0]])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\narray([1.        , 1.22314355, 1.51082562, 1.        , 1.91629073,\\n       1.        , 1.91629073, 1.91629073])\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe['tfid'].idf_\n",
    "\"\"\"\n",
    "array([1.        , 1.22314355, 1.51082562, 1.        , 1.91629073,\n",
    "       1.        , 1.91629073, 1.91629073])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n(4, 8)\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.transform(corpus).shape\n",
    "\n",
    "\"\"\"\n",
    "(4, 8)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以《中国的蜜蜂养殖》为例，假定该文长度为1000个词，\"中国\"、\"蜜蜂\"、\"养殖\"各出现20次，则这三个词的\"词频\"（TF）都为0.02。然后，搜索Google发现，包含\"的\"字的网页共有250亿张，假定这就是中文网页总数。包含\"中国\"的网页共有62.3亿张，包含\"蜜蜂\"的网页为0.484亿张，包含\"养殖\"的网页为0.973亿张。则它们的逆文档频率（IDF）和TF-IDF如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf=0.02, idf=0.6035, tfidf=0.0121\n",
      "tf=0.02, idf=2.7131, tfidf=0.0543\n",
      "tf=0.02, idf=2.4098, tfidf=0.0482\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def tfidf(wtermfreq, alltermfreq, wdocfreq, alldocfreq):\n",
    "    tf = round(wtermfreq/alltermfreq,4)\n",
    "    idf = round(math.log10(wdocfreq/(alldocfreq+1)),4)\n",
    "    tfidf = round(tf*idf,4) \n",
    "    print (\"tf=\"+str(tf)+\",\",\"idf=\"+str(idf)+\",\",\"tfidf=\"+str(tfidf))\n",
    "\n",
    "#中国\n",
    "tfidf(20,1000,250*10**8,62.3*10**8)\n",
    "\n",
    "#蜜蜂\n",
    "tfidf(20,1000,250*10**8,0.484*10**8)\n",
    "\n",
    "#养殖\n",
    "tfidf(20,1000,250*10**8,0.973*10**8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上表可见，\"蜜蜂\"的TF-IDF值最高，\"养殖\"其次，\"中国\"最低。（如果还计算\"的\"字的TF-IDF，那将是一个极其接近0的值。）所以，如果只选择一个词，\"蜜蜂\"就是这篇文章的关键词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了自动提取关键词，TF-IDF算法还可以用于许多别的地方。比如，信息检索时，对于每个文档，都可以分别计算一组搜索词（\"中国\"、\"蜜蜂\"、\"养殖\"）的TF-IDF，将它们相加，就可以得到整个文档的TF-IDF。这个值最高的文档就是与搜索词最相关的文档。\n",
    "TF-IDF算法的优点是简单快速，结果比较符合实际情况。缺点是，单纯以\"词频\"衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。（一种解决方法是，对全文的第一段和每一段的第一句话，给予较大的权重。）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本章小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章我们介绍了基于机器学习的文本分类方法，并完成了两种方法的对比。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本章作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 尝试改变TF-IDF的参数，并验证精度\n",
    "2. 尝试使用其他机器学习模型，完成训练和验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 尝试改变TF-IDF的参数，并验证精度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "看paramater部分，调参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1. \n",
    "TF-IDF + RidgeClassifier \n",
    "把总数据量调大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "df_train1=pd.read_csv('C:/Chen/1_Coding/20200721_nlp_learning/train_set.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9016443781346356\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=3000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. \n",
    "TF-IDF + RidgeClassifier \n",
    "在1.1的基础上把max_feature调大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. \n",
    "TF-IDF + RidgeClassifier \n",
    "在1.1的基础上把ngram_range和max_feature调大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,4), max_features=4000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.尝试使用其他机器学习模型，完成训练和验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类器总结\n",
    "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + LR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + svm.linearSVR\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = LinearSVR()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + svm.SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + knn_classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + DT_classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + GBDT_classifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = GradientBoostingRegressor()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + RF_classifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = RandomForestRegressor()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + Xgboost_classifier\n",
    "from sklearn.tree import RandomForestRegressor\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = RandomForestRegressor()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = MLPClassifier()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + svm.linearSVR(gridsearch)\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=20000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "parameters = {\"gamma\":[0.001,0.01,0.1,1,10,100], \"C\":[0.001,0.01,0.1,1,10,100]}\n",
    "clf = LinearSVR(gamma, C, cv=5)\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "clf.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LinearSVR(gamma=x, C=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_pred = clf.predict(train_test[:])\n",
    "#print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
