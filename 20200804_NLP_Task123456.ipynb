{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1 + Task2 + Task3 +Task4+Task5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of this competition: \n",
    "\n",
    "- predict the category of each news. \n",
    "\n",
    "\n",
    "Evaluation: \n",
    "\n",
    "- Avg value of f1_score. \n",
    "\n",
    "- A comparation between submitting category result and real category data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12281897.0.0.7a212b4amWToWd&postId=95703\n",
    "\n",
    "Task1 赛题理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.9.6406111aIKCSLV&postId=118253\n",
    "\n",
    "Task2 数据读取与数据分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12281897.0.0.7a212b4amWToWd&postId=118254\n",
    "\n",
    "https://github.com/datawhalechina/team-learning-nlp/blob/master/NewsTextClassification/Task3%20%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.md\n",
    "\n",
    "Task3 基于机器学习的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12281897.0.0.7a212b4amWToWd&postId=118255\n",
    "\n",
    "Task4 基于深度学习的文本分类1-fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tianchi.aliyun.com/notebook-ai/?spm=5176.12282024.0.0.432d14ca2Inpak\n",
    "\n",
    "https://github.com/datawhalechina/team-learning-nlp/tree/master/NewsTextClassification\n",
    "    \n",
    "All Task1-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://shimo.im/docs/rp3OVnXzDWtpoxAm/read\n",
    "    \n",
    "石墨文档大纲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**听课笔记**\n",
    "- Part1 比赛介绍\n",
    "- Part2 baseline\n",
    "- Part3 比赛知识点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个进行文本分类的数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么匿名数据集？\n",
    "- 没有版权问题；\n",
    "- 需要从头构建词向量。（不能分词）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思路：（四种方法，由易到难）\n",
    "- TF-IDF + RidgeClassifier ; (F1_score ~ 0.93)\n",
    "- FastText ; (F1_score ~ 0.93-0.94)\n",
    "- Word2Vec + TextCNN ; (F1_score ~ 0.95-0.97)\n",
    "- Bert.(F1_score ~ 0.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思路1 \n",
    "1 ：将字符进行 TF-IDF (term frequency–inverse document frequency）统计，然后送入线性分类器进行训练；\n",
    "- CountVectorizer RidgeClassifier\n",
    "- TfidfVectorizer RidgeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。tf-idf加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了tf-idf以外，互联网上的搜索引擎还会使用基于链接分析的评级方法，以确定文件在搜索结果中出现的顺序。)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline\n",
    "- 如何继续深入，提高精度？\n",
    "- 尝试其他机器学习模型；\n",
    "- 对 TF IDF 和 ngram 进行 gridsearch\n",
    "- 尝试思路 2 、思路 3 、思路 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其它尝试和思考：\n",
    "你能分析得出匿名字符中的标点符号吗？ （可以出现结尾，但不开头）\n",
    "你知道 NLP 中哪些数据扩增方法呢？（同义词替换，添加某个单词，等等）\n",
    "线上 F1 打分能到 0.99 吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding and exploratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testa=pd.read_csv('C:/Chen/1_Coding/20200721_nlp_learning/test_a.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('C:/Chen/1_Coding/20200721_nlp_learning/train_set.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5399 3117 1070 4321 4568 2621 5466 3772 4516 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2491 4109 1757 7539 648 3695 3038 4490 23 7019...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2673 5076 6835 2835 5948 5677 3247 4124 2465 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4562 4893 2210 4761 3659 1324 2595 5949 4583 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4269 7134 2614 1724 4464 1324 3370 3370 2106 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>3725 4498 2282 1647 6293 4245 4498 3615 1141 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>4811 465 3800 1394 3038 2376 2327 5165 3070 57...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>5338 1952 3117 4109 299 6656 6654 3792 6831 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>893 3469 5775 584 2490 4223 6569 6663 2124 168...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>2400 4409 4412 2210 5122 4464 7186 2465 1327 9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      5399 3117 1070 4321 4568 2621 5466 3772 4516 2...\n",
       "1      2491 4109 1757 7539 648 3695 3038 4490 23 7019...\n",
       "2      2673 5076 6835 2835 5948 5677 3247 4124 2465 5...\n",
       "3      4562 4893 2210 4761 3659 1324 2595 5949 4583 2...\n",
       "4      4269 7134 2614 1724 4464 1324 3370 3370 2106 2...\n",
       "49995  3725 4498 2282 1647 6293 4245 4498 3615 1141 2...\n",
       "49996  4811 465 3800 1394 3038 2376 2327 5165 3070 57...\n",
       "49997  5338 1952 3117 4109 299 6656 6654 3792 6831 21...\n",
       "49998  893 3469 5775 584 2490 4223 6569 6663 2124 168...\n",
       "49999  2400 4409 4412 2210 5122 4464 7186 2465 1327 9..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testa.head().append(df_testa.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2967 6758 339 2021 1854 3731 4109 3792 4149 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>4464 486 6352 5619 2465 4802 1452 3137 5778 54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7346 4068 5074 3747 5681 6093 1777 2226 7354 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>7159 948 4866 2109 5520 2490 211 3956 5520 549...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3646 3055 3055 2490 4659 6065 3370 5814 2465 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>2</td>\n",
       "      <td>307 4894 7539 4853 5330 648 6038 4409 3764 603...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>2</td>\n",
       "      <td>3792 2983 355 1070 4464 5050 6298 3782 3130 68...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>11</td>\n",
       "      <td>6811 1580 7539 1252 1899 5139 1386 3870 4124 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>2</td>\n",
       "      <td>6405 3203 6644 983 794 1913 1678 5736 1397 191...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>3</td>\n",
       "      <td>4350 3878 3268 1699 6909 5505 2376 2465 6088 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                               text\n",
       "0           2  2967 6758 339 2021 1854 3731 4109 3792 4149 15...\n",
       "1          11  4464 486 6352 5619 2465 4802 1452 3137 5778 54...\n",
       "2           3  7346 4068 5074 3747 5681 6093 1777 2226 7354 6...\n",
       "3           2  7159 948 4866 2109 5520 2490 211 3956 5520 549...\n",
       "4           3  3646 3055 3055 2490 4659 6065 3370 5814 2465 5...\n",
       "199995      2  307 4894 7539 4853 5330 648 6038 4409 3764 603...\n",
       "199996      2  3792 2983 355 1070 4464 5050 6298 3782 3130 68...\n",
       "199997     11  6811 1580 7539 1252 1899 5139 1386 3870 4124 1...\n",
       "199998      2  6405 3203 6644 983 794 1913 1678 5736 1397 191...\n",
       "199999      3  4350 3878 3268 1699 6909 5505 2376 2465 6088 2..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head().append(df_train.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 2 columns):\n",
      "label    200000 non-null int64\n",
      "text     200000 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 1 columns):\n",
      "text    50000 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 390.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_testa.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.210950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.084955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label\n",
       "count  200000.000000\n",
       "mean        3.210950\n",
       "std         3.084955\n",
       "min         0.000000\n",
       "25%         1.000000\n",
       "50%         2.000000\n",
       "75%         5.000000\n",
       "max        13.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2538 2506 1363 5466 3772 340 922 433 2397 5778...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "count                                               50000\n",
       "unique                                              49995\n",
       "top     2538 2506 1363 5466 3772 340 922 433 2397 5778...\n",
       "freq                                                    4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testa.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "count    200000.000000\n",
      "mean        907.207110\n",
      "std         996.029036\n",
      "min           2.000000\n",
      "25%         374.000000\n",
      "50%         676.000000\n",
      "75%        1131.000000\n",
      "max       57921.000000\n",
      "Name: text_len, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#In order to know the length of sentences\n",
    "#The number of words in sentence\n",
    "%pylab inline\n",
    "df_train['text_len'] = df_train['text'].apply(lambda x: len(x.split(' ')))\n",
    "print(df_train['text_len'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In resume, this competition gives relatively long sentences. A sentence is composed by 907 words in average, with the minimum length of 2, and the maximum length of 44665 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viz of sentences' length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram of char count')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Viz of sentences' length\n",
    "_ = plt.hist(df_train['text_len'], bins=200)\n",
    "plt.xlabel('Text char count')\n",
    "plt.title(\"Histogram of char count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viz of news category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'category')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Viz of news category\n",
    "df_train['label'].value_counts().plot(kind='bar')\n",
    "plt.title('News class count')\n",
    "plt.xlabel(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在数据集中标签的对应的关系如下：{'科技': 0, '股票': 1, '体育': 2, '娱乐': 3, '时政': 4, '社会': 5, '教育': 6, '财经': 7, '家居': 8, '游戏': 9, '房产': 10, '时尚': 11, '彩票': 12, '星座': 13}\n",
    "从统计结果可以看出，赛题的数据集类别分布存在较为不均匀的情况。在训练集中科技类新闻最多，其次是股票类新闻，最少的新闻是星座新闻。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequencty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the most/least frequent word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lines = ' '.join(list(df_train['text']))\n",
    "type(all_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter(all_lines.split(\" \"))\n",
    "word_count = sorted(word_count.items(), key=lambda d:int(d[1]), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6869\n"
     ]
    }
   ],
   "source": [
    "print(len(word_count))\n",
    "# in total there are 6869 words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('3750', 7482224)\n",
      "('648', 4924890)\n",
      "('900', 3262544)\n",
      "('3370', 2020958)\n",
      "('6122', 1602363)\n"
     ]
    }
   ],
   "source": [
    "print(word_count[0])\n",
    "print(word_count[1])\n",
    "print(word_count[2])\n",
    "print(word_count[3])\n",
    "print(word_count[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('3133', 1)\n"
     ]
    }
   ],
   "source": [
    "print(word_count[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find punctuation marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里还可以根据字在每个句子的出现情况，反推出标点符号。下面代码统计了不同字符在句子中出现的次数，其中字符3750，字符900和字符648在20w新闻的覆盖率接近99%，很有可能是标点符号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text_unique'] = df_train['text'].apply(lambda x: ' ' .join(list(set(x.split(' ')))))\n",
    "#df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines1 = ' '.join(list(df_train['text_unique']))\n",
    "word_count1 = Counter(all_lines1.split(\" \"))\n",
    "word_count1 = sorted(word_count1.items(), key=lambda d:int(d[1]), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('3750', 197997)\n",
      "('900', 197653)\n",
      "('648', 191975)\n",
      "('2465', 177310)\n",
      "('6122', 176543)\n"
     ]
    }
   ],
   "source": [
    "print(word_count1[0])\n",
    "print(word_count1[1])\n",
    "print(word_count1[2])\n",
    "print(word_count1[3])\n",
    "print(word_count1[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据分析的结论\n",
    "通过上述分析我们可以得出以下结论：\n",
    "\n",
    "赛题中每个新闻包含的字符个数平均为1000个，还有一些新闻字符较长；\n",
    "赛题中新闻类别分布不均匀，科技类新闻样本量接近4w，星座类新闻样本量不到1k；\n",
    "赛题总共包括7000-8000个字符；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过数据分析，我们还可以得出以下结论：\n",
    "\n",
    "每个新闻平均字符个数较多，可能需要截断；\n",
    "\n",
    "由于类别不均衡，会严重影响模型的精度；\n",
    "\n",
    "本章小结\n",
    "本章对赛题数据进行读取，并新闻句子长度、类别和字符进行了可视化分析。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章作业\n",
    "假设字符3750，字符900和字符648是句子的标点符号，请分析赛题每篇新闻平均由多少个句子构成？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_count[0][0])\n",
    "#word_count[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5298"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_number = int(word_count[0][0]) + int(word_count[1][0]) + int(word_count[2][0])\n",
    "sentence_number "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计每类新闻中出现次数对多的字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0     18587\n",
       "1     57921\n",
       "2     41894\n",
       "3     10817\n",
       "4     14928\n",
       "5     15839\n",
       "6     25728\n",
       "7     14469\n",
       "8     15271\n",
       "9     23866\n",
       "10    20622\n",
       "11     5729\n",
       "12     8737\n",
       "13     6399\n",
       "Name: text_len, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df_train.groupby(['label'], sort=True)['text_len'].max()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task3 基于机器学习的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一章节，我们对赛题的数据进行了读取，并在末尾给出了两个小作业。如果你顺利完成了作业，那么你基本上对`Python`也比较熟悉了。在本章我们将使用传统机器学习算法来完成新闻分类的过程，将会结束到赛题的核心知识点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于机器学习的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本章我们将开始使用机器学习模型来解决文本分类。机器学习发展比较广，且包括多个分支，本章侧重使用传统机器学习，从下一章开始是基于深度学习的文本分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 学会TF-IDF的原理和使用\n",
    "- 使用sklearn的机器学习模型完成文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 机器学习模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "机器学习是对能通过经验自动改进的计算机算法的研究。机器学习通过历史数据**训练**出**模型**对应于人类对经验进行**归纳**的过程，机器学习利用**模型**对新数据进行**预测**对应于人类利用总结的**规律**对新问题进行**预测**的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习有很多种分支，对于学习者来说应该优先掌握机器学习算法的分类，然后再其中一种机器学习算法进行学习。由于机器学习算法的分支和细节实在是太多，所以如果你一开始就被细节迷住了眼，你就很难知道全局是什么情况的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你是机器学习初学者，你应该知道如下的事情：\n",
    "\n",
    "1. 机器学习能解决一定的问题，但不能奢求机器学习是万能的；\n",
    "2. 机器学习算法有很多种，看具体问题需要什么，再来进行选择；\n",
    "3. 每种机器学习算法有一定的偏好，需要具体问题具体分析；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![machine_learning_overview](https://img-blog.csdnimg.cn/20200714203223253.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本表示方法 Part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在机器学习算法的训练过程中，假设给定$N$个样本，每个样本有$M$个特征，这样组成了$N×M$的样本矩阵，然后完成算法的训练和预测。同样的在计算机视觉中可以将图片的像素看作特征，每张图片看作hight×width×3的特征图，一个三维的矩阵来进入计算机进行计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是在自然语言领域，上述方法却不可行：文本是不定长度的。文本表示成计算机能够运算的数字或向量的方法一般称为词嵌入（Word Embedding）方法。词嵌入将不定长的文本转换到定长的空间内，是文本分类的第一步。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的One-hot与数据挖掘任务中的操作是一致的，即将每一个单词使用一个离散的向量表示。具体将每个字/词编码一个索引，然后根据索引进行赋值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot表示方法的例子如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "句子1：我 爱 北 京 天 安 门\n",
    "句子2：我 喜 欢 上 海\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先对所有句子的字进行索引，即将每个字确定一个编号："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "{\n",
    "\t'我': 1, '爱': 2, '北': 3, '京': 4, '天': 5,\n",
    "  '安': 6, '门': 7, '喜': 8, '欢': 9, '上': 10, '海': 11\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里共包括11个字，因此每个字可以转换为一个11维度稀疏向量："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "我：[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "爱：[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "...\n",
    "海：[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words（词袋表示），也称为Count Vectors，每个文档的字/词可以使用其出现次数来进行表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "句子1：我 爱 北 京 天 安 门\n",
    "句子2：我 喜 欢 上 海\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接统计每个字出现的次数，并进行赋值："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "句子1：我 爱 北 京 天 安 门\n",
    "转换为 [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "\n",
    "句子2：我 喜 欢 上 海\n",
    "转换为 [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在sklearn中可以直接`CountVectorizer`来实现这一步骤："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer() \n",
    "vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "# CountVectorizer() : Get the term frequency\n",
    "# n_gram好像是用来组词的，几个词一组？\n",
    "#大小写不敏感\n",
    "#出现的英文词，按alphabet重排序了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram与Count Vectors类似，不过加入了相邻单词组合成为新的单词，并进行计数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果N取值为2，则句子1和句子2就变为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "句子1：我爱 爱北 北京 京天 天安 安门\n",
    "句子2：我喜 喜欢 欢上 上海\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF 分数由两部分组成：第一部分是**词语频率**（Term Frequency），第二部分是**逆文档频率**（Inverse Document Frequency）。其中计算语料库中文档总数除以含有该词语的文档数量，然后再取对数就是逆文档频率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TF(t)= 该词语在当前文档出现的次数 / 当前文档中词语的总数\n",
    "IDF(t)= log_e（文档总数 / 出现该词语的文档总数）\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于机器学习的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们将对比不同文本表示算法的精度，通过本地构建验证集计算F1得分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectors + RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import f1_score \n",
    "\n",
    "\n",
    "df_train=pd.read_csv('C:/Chen/1_Coding/20200721_nlp_learning/train_set.csv',sep='\\t', nrows=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65441877581244\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=3000)\n",
    "train_test = vectorizer.fit_transform(df_train['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:10000], df_train['label'].values[:10000])\n",
    "\n",
    "val_pred = clf.predict(train_test[10000:])\n",
    "print(f1_score(df_train['label'].values[10000:], val_pred, average='macro'))\n",
    "#0.654430246247168"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  TF-IDF +  RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "df_train=pd.read_csv('C:/Chen/1_Coding/20200721_nlp_learning/train_set.csv',sep='\\t', nrows=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8719098297954606\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=3000)\n",
    "train_test = tfidf.fit_transform(df_train['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:10000], df_train['label'].values[:10000])\n",
    "\n",
    "val_pred = clf.predict(train_test[10000:])\n",
    "print(f1_score(df_train['label'].values[10000:], val_pred, average='macro'))\n",
    "\n",
    "# CountVectorizer() : Get the term frequency\n",
    "# TfidfVectorizer() : based on CountVectorizer() \n",
    "# 需要先用CountVectorizer()\n",
    "#0.8719372173702"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**以下是一个来自Sklearn官方文档的例子**\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "\n",
    "另外还有两个相关文档：\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\narray([[1, 1, 1, 1, 0, 1, 0, 0],\\n       [1, 2, 0, 1, 1, 1, 0, 0],\\n       [1, 0, 0, 1, 0, 1, 1, 1],\\n       [1, 1, 1, 1, 0, 1, 0, 0]])\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "corpus = ['this is the first document',\n",
    "          'this document is the second document',\n",
    "          'and this is the third one',\n",
    "          'is this the first document']\n",
    "vocabulary = ['this', 'document', 'first', 'is', 'second', 'the', 'and', 'one']\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)), \n",
    "                 # CountVectorizer() : 计算文档中所有包括该词的数目\n",
    "                 ('tfid', TfidfTransformer())]).fit(corpus)\n",
    "pipe['count'].transform(corpus).toarray()\n",
    "\"\"\"\n",
    "array([[1, 1, 1, 1, 0, 1, 0, 0],\n",
    "       [1, 2, 0, 1, 1, 1, 0, 0],\n",
    "       [1, 0, 0, 1, 0, 1, 1, 1],\n",
    "       [1, 1, 1, 1, 0, 1, 0, 0]])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\narray([1.        , 1.22314355, 1.51082562, 1.        , 1.91629073,\\n       1.        , 1.91629073, 1.91629073])\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe['tfid'].idf_\n",
    "\"\"\"\n",
    "array([1.        , 1.22314355, 1.51082562, 1.        , 1.91629073,\n",
    "       1.        , 1.91629073, 1.91629073])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n(4, 8)\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.transform(corpus).shape\n",
    "\n",
    "\"\"\"\n",
    "(4, 8)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以《中国的蜜蜂养殖》为例，假定该文长度为1000个词，\"中国\"、\"蜜蜂\"、\"养殖\"各出现20次，则这三个词的\"词频\"（TF）都为0.02。然后，搜索Google发现，包含\"的\"字的网页共有250亿张，假定这就是中文网页总数。包含\"中国\"的网页共有62.3亿张，包含\"蜜蜂\"的网页为0.484亿张，包含\"养殖\"的网页为0.973亿张。则它们的逆文档频率（IDF）和TF-IDF如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf=0.02, idf=0.6035, tfidf=0.0121\n",
      "tf=0.02, idf=2.7131, tfidf=0.0543\n",
      "tf=0.02, idf=2.4098, tfidf=0.0482\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def tfidf(wtermfreq, alltermfreq, wdocfreq, alldocfreq):\n",
    "    tf = round(wtermfreq/alltermfreq,4)\n",
    "    idf = round(math.log10(wdocfreq/(alldocfreq+1)),4)\n",
    "    tfidf = round(tf*idf,4) \n",
    "    print (\"tf=\"+str(tf)+\",\",\"idf=\"+str(idf)+\",\",\"tfidf=\"+str(tfidf))\n",
    "\n",
    "#中国\n",
    "tfidf(20,1000,250*10**8,62.3*10**8)\n",
    "\n",
    "#蜜蜂\n",
    "tfidf(20,1000,250*10**8,0.484*10**8)\n",
    "\n",
    "#养殖\n",
    "tfidf(20,1000,250*10**8,0.973*10**8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上表可见，\"蜜蜂\"的TF-IDF值最高，\"养殖\"其次，\"中国\"最低。（如果还计算\"的\"字的TF-IDF，那将是一个极其接近0的值。）所以，如果只选择一个词，\"蜜蜂\"就是这篇文章的关键词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了自动提取关键词，TF-IDF算法还可以用于许多别的地方。比如，信息检索时，对于每个文档，都可以分别计算一组搜索词（\"中国\"、\"蜜蜂\"、\"养殖\"）的TF-IDF，将它们相加，就可以得到整个文档的TF-IDF。这个值最高的文档就是与搜索词最相关的文档。\n",
    "TF-IDF算法的优点是简单快速，结果比较符合实际情况。缺点是，单纯以\"词频\"衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。（一种解决方法是，对全文的第一段和每一段的第一句话，给予较大的权重。）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本章小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章我们介绍了基于机器学习的文本分类方法，并完成了两种方法的对比。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本章作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 尝试改变TF-IDF的参数，并验证精度\n",
    "2. 尝试使用其他机器学习模型，完成训练和验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 尝试改变TF-IDF的参数，并验证精度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "看paramater部分，调参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1. \n",
    "TF-IDF + RidgeClassifier \n",
    "把总数据量调大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "df_train1=pd.read_csv('C:/Chen/1_Coding/20200721_nlp_learning/train_set.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9016880865055513\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=3000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. \n",
    "TF-IDF + RidgeClassifier \n",
    "在1.1的基础上把max_feature调大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9520909774028781\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9520985727964899\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出预测结果\n",
    "test_a = pd.read_csv('C:/Chen/1_Coding/20200721_nlp_learning/test_a.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a_tfidf = tfidf.fit_transform(test_a['text'])\n",
    "df = pd.Dataframe()\n",
    "df['label'] = clf.predict(test_a_tfidf)\n",
    "df_to_csv('submit.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. \n",
    "TF-IDF + RidgeClassifier \n",
    "在1.1的基础上把ngram_range和max_feature调大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9099982540691449\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,4), max_features=4000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.尝试使用其他机器学习模型，完成训练和验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类器总结\n",
    "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xingy\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\xingy\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9383878432526526\n"
     ]
    }
   ],
   "source": [
    "#tfidf + LR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-0a13b06521ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mval_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m   1057\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[0;32m   1058\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1059\u001b[1;33m                        sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m   1180\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f-score'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1182\u001b[1;33m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1183\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[0;32m   1413\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[1;32m-> 1415\u001b[1;33m                                     pos_label)\n\u001b[0m\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[1;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                          str(average_options))\n\u001b[0;32m   1238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1240\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'binary'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 81\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous targets"
     ]
    }
   ],
   "source": [
    "#tfidf + svm.linearSVR\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = LinearSVR()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + svm.SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + knn_classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + DT_classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + GBDT_classifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = GradientBoostingRegressor()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + RF_classifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = RandomForestRegressor()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + Xgboost_classifier\n",
    "from sklearn.tree import RandomForestRegressor\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = RandomForestRegressor()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = MLPClassifier()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + svm.linearSVR(gridsearch)\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=20000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "parameters = {\"gamma\":[0.001,0.01,0.1,1,10,100], \"C\":[0.001,0.01,0.1,1,10,100]}\n",
    "clf = LinearSVR(gamma, C, cv=5)\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "clf.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LinearSVR(gamma=x, C=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_pred = clf.predict(train_test[:])\n",
    "#print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task4 基于深度学习的文本分类1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一章节，我们使用传统机器学习算法来解决了文本分类问题，从本章开始我们将尝试使用深度学习方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于深度学习的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与传统机器学习不同，深度学习既提供特征提取功能，也可以完成分类的功能。从本章开始我们将学习如何使用深度学习来完成文本表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 学习FastText的使用和基础原理\n",
    "- 学会使用验证集进行调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本表示方法 Part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 现有文本表示方法的缺陷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一章节，我们介绍几种文本表示方法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One-hot\n",
    "- Bag of Words\n",
    "- N-gram\n",
    "- TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也通过sklean进行了相应的实践，相信你也有了初步的认知。但上述方法都或多或少存在一定的问题：转换得到的向量维度很高，需要较长的训练实践；没有考虑单词与单词之间的关系，只是进行了统计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与这些表示方法不同，深度学习也可以用于文本表示，还可以将其映射到一个低纬空间。其中比较典型的例子有：FastText、Word2Vec和Bert。在本章我们将介绍FastText，将在后面的内容介绍Word2Vec和Bert。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText是一种典型的深度学习词向量的表示方法，它非常简单通过Embedding层将单词映射到稠密空间，然后将句子中所有的单词在Embedding空间中进行平均，进而完成分类操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以FastText是一个三层的神经网络，输入层、隐含层和输出层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fast_text](https://img-blog.csdnimg.cn/20200714204856589.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图是使用keras实现的FastText网络结构："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![keras_fasttext](https://img-blog.csdnimg.cn/20200714204249463.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText在文本分类任务上，是优于TF-IDF的："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FastText用单词的Embedding叠加获得的文档向量，将相似的句子分为一类\n",
    "- FastText学习到的Embedding空间维度比较低，可以快速进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想深度学习，可以参考论文："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Tricks for Efficient Text Classification, https://arxiv.org/abs/1607.01759"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于FastText的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText可以快速的在CPU上进行训练，最好的实践方法就是官方开源的版本：\n",
    "https://github.com/facebookresearch/fastText/tree/master/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pip安装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pip install fasttext\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "conda install -c mbednarski fasttext\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 源码安装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "git clone https://github.com/facebookresearch/fastText.git\n",
    "cd fastText\n",
    "sudo pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两种安装方法都可以安装，如果你是初学者可以优先考虑使用pip安装。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 转换为FastText需要的格式\n",
    "df_train = pd.read_csv('C:/Chen/1_Coding/20200721_nlp_learning/train_set.csv', sep='\\t', nrows=15000)\n",
    "df_train['label_ft'] = '__label__' + df_train['label'].astype(str)\n",
    "df_train[['text','label_ft']].iloc[:-5000].to_csv('train.csv', index=None, header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8216295413400136\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised('train.csv', lr=1.0, wordNgrams=2, \n",
    "                                  verbose=2, minCount=1, epoch=25, loss=\"hs\")\n",
    "\n",
    "val_pred = [model.predict(x)[0][0].split('__')[-1] for x in df_train.iloc[-5000:]['text']]\n",
    "print(f1_score(df_train['label'].values[-5000:].astype(str), val_pred, average='macro'))\n",
    "# 0.8238894886303253"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时数据量比较小得分为0.82，当不断增加训练集数量时，FastText的精度也会不断增加5w条训练样本时，验证集得分可以到0.89-0.90左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9097346804322758\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 转换为FastText需要的格式\n",
    "df_train = pd.read_csv('C:/Chen/1_Coding/20200721_nlp_learning/train_set.csv', sep='\\t') #, nrows=15000, 把数据加多\n",
    "df_train['label_ft'] = '__label__' + df_train['label'].astype(str)\n",
    "df_train[['text','label_ft']].iloc[:-5000].to_csv('train.csv', index=None, header=None, sep='\\t')\n",
    "\n",
    "\n",
    "# 用FastText建模型\n",
    "import fasttext\n",
    "\n",
    "model = fasttext.train_supervised('train.csv', lr=1.0, wordNgrams=2, \n",
    "                                  verbose=2, minCount=1, epoch=25, loss=\"hs\")\n",
    "\n",
    "val_pred = [model.predict(x)[0][0].split('__')[-1] for x in df_train.iloc[-5000:]['text']]\n",
    "print(f1_score(df_train['label'].values[-5000:].astype(str), val_pred, average='macro'))\n",
    "# 0.9097346804322758\n",
    "# 好像用了至少半个小时"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何使用验证集调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用TF-IDF和FastText中，有一些模型的参数需要选择，这些参数会在一定程度上影响模型的精度，那么如何选择这些参数呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过阅读文档，要弄清楚这些参数的大致含义，那些参数会增加模型的复杂度\n",
    "- 通过在验证集上进行验证模型精度，找到模型在是否过拟合还是欠拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![train_val](https://img-blog.csdnimg.cn/20200714204403844.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们使用10折交叉验证，每折使用9/10的数据进行训练，剩余1/10作为验证集检验模型的效果。这里需要注意每折的划分必须保证标签的分布与整个数据集的分布一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "label2id = {}\n",
    "for i in range(total):\n",
    "    label = str(all_labels[i])\n",
    "    if label not in label2id:\n",
    "        label2id[label] = [i]\n",
    "    else:\n",
    "        label2id[label].append(i)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过10折划分，我们一共得到了10份分布一致的数据，索引分别为0到9，每次通过将一份数据作为验证集，剩余数据作为训练集，获得了所有数据的10种分割。不失一般性，我们选择最后一份完成剩余的实验，即索引为9的一份做为验证集，索引为1-8的作为训练集，然后基于验证集的结果调整超参数，使得模型性能更优。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本章小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章介绍了FastText的原理和基础使用，并进行相应的实践。然后介绍了通过10折交叉验证划分数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本章作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 阅读FastText的文档，尝试修改参数，得到更好的分数\n",
    "- 基于验证集的结果调整超参数，使得模型性能更优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文档在这\n",
    "https://pypi.org/project/fasttext/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.1, dim=100, minCount=2\n",
      "0.9856942272723739\n",
      "lr=0.1, dim=100, minCount=3\n",
      "0.9859750049975563\n",
      "lr=0.1, dim=100, minCount=4\n",
      "0.9859750049975563\n",
      "lr=0.1, dim=200, minCount=2\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('C:/Chen/1_Coding/20200721_nlp_learning/train_set.csv', sep='\\t', nrows=15000) \n",
    "\n",
    "import fasttext\n",
    "\n",
    "for lr in [0.1, 0.5, 1]: \n",
    "    for dim in [100, 200, 300]: \n",
    "        for minCount in [2, 3, 4]: \n",
    "            print(\"lr=%.1f, dim=%d, minCount=%d\"%(lr,dim,minCount))\n",
    "            model = fasttext.train_supervised('train.csv', lr=lr, wordNgrams=2, dim=dim, minCount=minCount, epoch=50, loss='hs')\n",
    "            val_pred = [model.predict(x)[0][0].split('__')[-1] for x in df_train.iloc[-5000:]['text']]\n",
    "            print(f1_score(df_train['label'].values[-5000:].astype(str), val_pred, average='macro'))\n",
    "\n",
    "\n",
    "#原来的参数\n",
    "\n",
    "#lr=1.0, \n",
    "#wordNgrams=2\n",
    "#verbose=2, \n",
    "#minCount=1, \n",
    "#epoch=25, \n",
    "#loss=\"hs\", \n",
    "#dim=100 # size of word vectors [100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task5 基于深度学习的文本分类2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一章节，我们通过FastText快速实现了基于深度学习的文本分类模型，但是这个模型并不是最优的。在本章我们将继续深入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于深度学习的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章将继续学习基于深度学习的文本分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 学习Word2Vec的使用和基础原理\n",
    "- 学习使用TextCNN、TextRNN进行文本表示\n",
    "- 学习使用HAN网络结构完成文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本表示方法 Part3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节通过word2vec学习词向量。word2vec模型背后的基本思想是对出现在上下文环境里的词进行预测。对于每一条输入文本，我们选取一个上下文窗口和一个中心词，并基于这个中心词去预测窗口里其他词出现的概率。因此，word2vec模型可以方便地从新增语料中学习到新增词的向量表达，是一种高效的在线学习算法（online learning）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec的主要思路：通过单词和上下文彼此预测，对应的两个算法分别为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Skip-grams (SG)：预测上下文\n",
    "* Continuous Bag of Words (CBOW)：预测目标单词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外提出两种更加高效的训练方法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hierarchical softmax\n",
    "\n",
    "* Negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Skip-grams原理和网络结构**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec模型中，主要有Skip-Gram和CBOW两种模型，从直观上理解，Skip-Gram是给定input word来预测上下文。而CBOW是给定上下文，来预测input word。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec模型中，主要有Skip-Gram和CBOW两种模型，从直观上理解，Skip-Gram是给定input word来预测上下文。而CBOW是给定上下文，来预测input word。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![skip_grams](https://img-blog.csdnimg.cn/20200714210354729.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec模型实际上分为了两个部分，第一部分为建立模型，第二部分是通过模型获取嵌入词向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵——后面我们将会看到这些权重在Word2Vec中实际上就是我们试图去学习的“word vectors”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-grams过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假如我们有一个句子“The dog barked at the mailman”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 首先我们选句子中间的一个词作为我们的输入词，例如我们选取“dog”作为input word；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 有了input word以后，我们再定义一个叫做skip_window的参数，它代表着我们从当前input word的一侧（左边或右边）选取词的数量。如果我们设置skip_window=2，那么我们最终获得窗口中的词（包括input word在内）就是['The', 'dog'，'barked', 'at']。skip_window=2代表着选取左input word左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小span=2x2=4。另一个参数叫num_skips，它代表着我们从整个窗口中选取多少个不同的词作为我们的output word，当skip_window=2，num_skips=2时，我们将会得到两组 (input word, output word) 形式的训练数据，即 ('dog', 'barked')，('dog', 'the')。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 神经网络基于这些训练数据将会输出一个概率分布，这个概率代表着我们的词典中的每个词作为input word的output word的可能性。这句话有点绕，我们来看个例子。第二步中我们在设置skip_window和num_skips=2的情况下获得了两组训练数据。假如我们先拿一组数据 ('dog', 'barked') 来训练神经网络，那么模型通过学习这个训练样本，会告诉我们词汇表中每个单词当'dog'作为input word时，其作为output word的可能性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是说模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现。例如：如果我们向神经网络模型中输入一个单词“Soviet“，那么最终模型的输出概率中，像“Union”， ”Russia“这种相关词的概率将远高于像”watermelon“，”kangaroo“非相关词的概率。因为”Union“，”Russia“在文本中更大可能在”Soviet“的窗口中出现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将通过给神经网络输入文本中成对的单词来训练它完成上面所说的概率计算。下面的图中给出了一些我们训练样本的例子。我们选定句子“The quick brown fox jumps over lazy dog”，设定我们的窗口大小为2（window_size=2），也就是说我们仅选输入词前后各两个词和输入词进行组合。下图中，蓝色代表input word，方框内代表位于窗口内的单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](https://img-blog.csdnimg.cn/20200721190035764.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2](https://img-blog.csdnimg.cn/20200714210519939.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的模型将会从每对单词出现的次数中习得统计结果。例如，我们的神经网络可能会得到更多类似（“Soviet“，”Union“）这样的训练样本对，而对于（”Soviet“，”Sasquatch“）这样的组合却看到的很少。因此，当我们的模型完成训练后，给定一个单词”Soviet“作为输入，输出的结果中”Union“或者”Russia“要比”Sasquatch“被赋予更高的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS：input word和output word都会被我们进行one-hot编码。仔细想一下，我们的输入被one-hot编码以后大多数维度上都是0（实际上仅有一个位置为1），所以这个向量相当稀疏，那么会造成什么结果呢。如果我们将一个1 x 10000的向量和10000 x 300的矩阵相乘，它会消耗相当大的计算资源，为了高效计算，它仅仅会选择矩阵中对应的向量中维度值为1的索引行："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdnimg.cn/20200714205344406.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Skip-grams训练**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上部分可知，Word2Vec模型是一个超级大的神经网络（权重矩阵规模非常大）。例如：我们拥有10000个单词的词汇表，我们如果想嵌入300维的词向量，那么我们的输入-隐层权重矩阵和隐层-输出层的权重矩阵都会有 10000 x 300 = 300万个权重，在如此庞大的神经网络中进行梯度下降是相当慢的。更糟糕的是，你需要大量的训练数据来调整这些权重并且避免过拟合。百万数量级的权重矩阵和亿万数量级的训练样本意味着训练这个模型将会是个灾难"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解决方案：\n",
    "\n",
    "* 将常见的单词组合（word pairs）或者词组作为单个“words”来处理\n",
    "\n",
    "* 对高频次单词进行抽样来减少训练样本的个数\n",
    "\n",
    "* 对优化目标采用“negative sampling”方法，这样每个训练样本的训练只会更新一小部分的模型权重，从而降低计算负担"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2.1 Word pairs and \"phases\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些单词组合（或者词组）的含义和拆开以后具有完全不同的意义。比如“Boston Globe”是一种报刊的名字，而单独的“Boston”和“Globe”这样单个的单词却表达不出这样的含义。因此，在文章中只要出现“Boston Globe”，我们就应该把它作为一个单独的词来生成其词向量，而不是将其拆开。同样的例子还有“New York”，“United Stated”等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Google发布的模型中，它本身的训练样本中有来自Google News数据集中的1000亿的单词，但是除了单个单词以外，单词组合（或词组）又有3百万之多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2.2 对高频词抽样*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一部分中，对于原始文本为“The quick brown fox jumps over the laze dog”，如果使用大小为2的窗口，那么我们可以得到图中展示的那些训练样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](https://img-blog.csdnimg.cn/20200714210458879.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是对于“the”这种常用高频单词，这样的处理方式会存在下面两个问题："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 当我们得到成对的单词训练样本时，(\"fox\", \"the\") 这样的训练样本并不会给我们提供关于“fox”更多的语义信息，因为“the”在每个单词的上下文中几乎都会出现\n",
    "\n",
    "2. 由于在文本中“the”这样的常用词出现概率很大，因此我们将会有大量的（”the“，…）这样的训练样本，而这些样本数量远远超过了我们学习“the”这个词向量所需的训练样本数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec通过“抽样”模式来解决这种高频词问题。它的基本思想如下：对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关。\n",
    "\n",
    "ωi 是一个单词，Z(ωi) 是 ωi 这个单词在所有语料中出现的频次，例如：如果单词“peanut”在10亿规模大小的语料中出现了1000次，那么 Z(peanut) = 1000/1000000000 = 1e - 6。\n",
    "\n",
    "P(ωi) 代表着保留某个单词的概率："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdnimg.cn/20200714205456898.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2.3 Negative sampling*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练一个神经网络意味着要输入训练样本并且不断调整神经元的权重，从而不断提高对目标的准确预测。每当神经网络经过一个训练样本的训练，它的权重就会进行一次调整。\n",
    "\n",
    "所以，词典的大小决定了我们的Skip-Gram神经网络将会拥有大规模的权重矩阵，所有的这些权重需要通过数以亿计的训练样本来进行调整，这是非常消耗计算资源的，并且实际中训练起来会非常慢。\n",
    "\n",
    "负采样（negative sampling）解决了这个问题，它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。\n",
    "\n",
    "当我们用训练样本 ( input word: \"fox\"，output word: \"quick\") 来训练我们的神经网络时，“ fox”和“quick”都是经过one-hot编码的。如果我们的词典大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为“negative” word。\n",
    "\n",
    "当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新（在我们上面的例子中，这个单词指的是”quick“）。\n",
    "\n",
    "PS: 在论文中，作者指出指出对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。\n",
    "\n",
    "我们使用“一元模型分布（unigram distribution）”来选择“negative words”。个单词被选作negative sample的概率跟它出现的频次有关，出现频次越高的单词越容易被选作negative words。\n",
    "\n",
    "每个单词被选为“negative words”的概率计算公式：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdnimg.cn/20200714205545327.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 f(ωi)代表着单词出现的频次，而公式中开3/4的根号完全是基于经验的。\n",
    "\n",
    "在代码负采样的代码实现中，unigram table有一个包含了一亿个元素的数组，这个数组是由词汇表中每个单词的索引号填充的，并且这个数组中有重复，也就是说有些单词会出现多次。那么每个单词的索引在这个数组中出现的次数该如何决定呢，有公式，也就是说计算出的负采样概率*1亿=单词在表中出现的次数。\n",
    "\n",
    "有了这张表以后，每次去我们进行负采样时，只需要在0-1亿范围内生成一个随机数，然后选择表中索引号为这个随机数的那个单词作为我们的negative word即可。一个单词的负采样概率越大，那么它在这个表中出现的次数就越多，它被选中的概率就越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Hierarchical Softmax**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3.1 霍夫曼树*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：权值为(w1,w2,…wn)的n个节点\n",
    "\n",
    "输出：对应的霍夫曼树\n",
    "\n",
    "1. 将(w1,w2,…wn)看做是有n棵树的森林，每个树仅有一个节点\n",
    "\n",
    "2. 在森林中选择根节点权值最小的两棵树进行合并，得到一个新的树，这两颗树分布作为新树的左右子树。新树的根节点权重为左右子树的根节点权重之和\n",
    "\n",
    "3. 将之前的根节点权值最小的两棵树从森林删除，并把新树加入森林\n",
    "\n",
    "4. 重复步骤 2 和 3 直到森林里只有一棵树为止"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们用一个具体的例子来说明霍夫曼树建立的过程，我们有(a，b，c，d，e，f)共6个节点，节点的权值分布是(16，4，8，6，20，3)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先是最小的b和f合并，得到的新树根节点权重是7.此时森林里5棵树，根节点权重分别是16，8，6，20，7。此时根节点权重最小的6，7合并，得到新子树，依次类推，最终得到下面的霍夫曼树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3](https://img-blog.csdnimg.cn/20200714210647687.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么霍夫曼树有什么好处呢？一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望越常用的词拥有更短的编码。如何编码呢？一般对于一个霍夫曼树的节点（根节点除外），可以约定左子树编码为0，右子树编码为1。如上图，则可以得到c的编码是00。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在word2vec中，约定编码方式和上面的例子相反，即约定左子树编码为1，右子树编码为0，同时约定左子树的权重不小于右子树的权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更多原理可参考：[霍夫曼树原理](https://blog.csdn.net/lzw66666/article/details/78934893)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3.2Hierarchical Softmax过程*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了避免要计算所有词的softmax概率，word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。\n",
    "\n",
    "霍夫曼树的建立：\n",
    "\n",
    "* 根据标签（label）和频率建立霍夫曼树（label出现的频率越高，Huffman树的路径越短）\n",
    "\n",
    "* Huffman树中每一叶子结点代表一个label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4](https://img-blog.csdnimg.cn/20200714205623583.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdnimg.cn/20200714205711676.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdnimg.cn/20200714205759860.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：此时的theta是一个待定系数，它是由推导最大似然之后求解得到迭代式子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdnimg.cn/20200714205841871.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**使用gensim训练word2vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, workers=num_workers, size=num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**参考：**\n",
    "\n",
    "1. [CS224n笔记2 词的向量表示：word2vec](http://www.hankcs.com/nlp/word-vector-representations-word2vec.html)\n",
    "\n",
    "2. [斯坦福大学深度学习与自然语言处理第二讲：词向量 ](http://www.52nlp.cn/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%AC%E4%BA%8C%E8%AE%B2%E8%AF%8D%E5%90%91%E9%87%8F)\n",
    "\n",
    "3. [(Stanford CS224d) Deep Learning and NLP课程笔记（三）：GloVe与模型的评估](https://www.cnblogs.com/iloveai/p/cs224d-lecture3-note.html)\n",
    "\n",
    "4. [http://www.cnblogs.com/pinard/p/7249903.html](http://www.cnblogs.com/pinard/p/7249903.html)\n",
    "\n",
    "5. [https://blog.csdn.net/yinkun6514/article/details/79218736](https://blog.csdn.net/yinkun6514/article/details/79218736)\n",
    "\n",
    "6. [https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html](https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextCNN利用CNN（卷积神经网络）进行文本特征抽取，不同大小的卷积核分别抽取n-gram特征，卷积计算出的特征图经过MaxPooling保留最大的特征值，然后将拼接成一个向量作为文本的表示。\n",
    "\n",
    "这里我们基于TextCNN原始论文的设定，分别采用了100个大小为2,3,4的卷积核，最后得到的文本向量大小为100*3=300维。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdnimg.cn/20200714205932720.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextRNN利用RNN（循环神经网络）进行文本特征抽取，由于文本本身是一种序列，而LSTM天然适合建模序列数据。TextRNN将句子中每个词的词向量依次输入到双向双层LSTM，分别将两个方向最后一个有效位置的隐藏层拼接成一个向量作为文本的表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![5](https://img-blog.csdnimg.cn/20200714210806492.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于TextCNN、TextRNN的文本表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextCNN利用CNN（卷积神经网络）进行文本特征抽取，不同大小的卷积核分别抽取n-gram特征，卷积计算出的特征图经过MaxPooling保留最大的特征值，然后将拼接成一个向量作为文本的表示。\n",
    "\n",
    "这里我们基于TextCNN原始论文的设定，分别采用了100个大小为2,3,4的卷积核，最后得到的文本向量大小为100*3=300维。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdnimg.cn/20200714205932720.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextRNN利用RNN（循环神经网络）进行文本特征抽取，由于文本本身是一种序列，而LSTM天然适合建模序列数据。TextRNN将句子中每个词的词向量依次输入到双向双层LSTM，分别将两个方向最后一个有效位置的隐藏层拼接成一个向量作为文本的表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![5](https://img-blog.csdnimg.cn/20200714210806492.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于TextCNN、TextRNN的文本表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 模型搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-da0f5229b63d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# n-gram window\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_channel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_channel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfilter_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfilter_size\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_sizes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "self.filter_sizes = [2, 3, 4]  # n-gram window\n",
    "self.out_channel = 100\n",
    "self.convs = nn.ModuleList([nn.Conv2d(1, self.out_channel, (filter_size, input_size), bias=True) for filter_size in self.filter_sizes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-98da35e8fd4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpooled_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mfilter_height\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent_len\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_sizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_embed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# sen_num x out_channel x filter_height x 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "pooled_outputs = []\n",
    "for i in range(len(self.filter_sizes)):\n",
    "    filter_height = sent_len - self.filter_sizes[i] + 1\n",
    "    conv = self.convs[i](batch_embed)\n",
    "    hidden = F.relu(conv)  # sen_num x out_channel x filter_height x 1\n",
    "\n",
    "    mp = nn.MaxPool2d((filter_height, 1))  # (filter_height, filter_width)\n",
    "    # sen_num x out_channel x 1 x 1 -> sen_num x out_channel\n",
    "    pooled = mp(hidden).reshape(sen_num, self.out_channel)\n",
    "    \n",
    "    pooled_outputs.append(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 模型搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_size = config.word_dims\n",
    "\n",
    "self.word_lstm = LSTM(\n",
    "    input_size=input_size,\n",
    "    hidden_size=config.word_hidden_size,\n",
    "    num_layers=config.word_num_layers,\n",
    "    batch_first=True,\n",
    "    bidirectional=True,\n",
    "    dropout_in=config.dropout_input,\n",
    "    dropout_out=config.dropout_hidden,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hiddens, _ = self.word_lstm(batch_embed, batch_masks)  # sent_len x sen_num x hidden*2\n",
    "hiddens.transpose_(1, 0)  # sen_num x sent_len x hidden*2\n",
    "\n",
    "if self.training:\n",
    "    hiddens = drop_sequence_sharedmask(hiddens, self.dropout_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用HAN用于文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hierarchical Attention Network for Document Classification](https://link.zhihu.com/?target=http%3A//www.aclweb.org/anthology/N16-1174)(HAN)基于层级注意力，在单词和句子级别分别编码并基于注意力获得文档的表示，然后经过Softmax进行分类。其中word encoder的作用是获得句子的表示，可以替换为上节提到的TextCNN和TextRNN，也可以替换为下节中的BERT。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sequence Intent Classification Using Hierarchical Attention...](https://img-blog.csdnimg.cn/20200714210015326.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本章小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章介绍了Word2Vec的使用，以及TextCNN、TextRNN的原理和训练，最后介绍了用于长文档分类的HAN。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本章作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 尝试通过Word2Vec训练词向量\n",
    "- 尝试使用TextCNN、TextRNN完成文本表示\n",
    "- 尝试使用HAN进行文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**参考：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://mp.weixin.qq.com/s/I-yeHQopTFdNk67Ir_iWiA\n",
    "2. https://github.com/hecongqing/2018-daguan-competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task6 基于深度学习的文本分类3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于深度学习的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 了解Transformer的原理和基于预训练语言模型（Bert）的词表示\n",
    "- 学会Bert的使用，具体包括pretrain和finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本表示方法Part4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer是在\"[Attention is All You Need](https://arxiv.org/abs/1706.03762)\"中提出的，模型的编码部分是一组编码器的堆叠（论文中依次堆叠六个编码器），模型的解码部分是由相同数量的解码器的堆叠。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdnimg.cn/20200714211046668.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们重点关注编码部分。他们结构完全相同，但是并不共享参数，每一个编码器都可以拆解成两部分。在对输入序列做词的向量化之后，它们首先流过一个self-attention层，该层帮助编码器在它编码单词的时候能够看到输入序列中的其他单词。self-attention的输出流向一个前向网络（Feed Forward Neural Network），每个输入位置对应的前向网络是独立互不干扰的。最后将输出传入下一个编码器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdnimg.cn/20200714211115945.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里能看到Transformer的一个关键特性，每个位置的词仅仅流过它自己的编码器路径。在self-attention层中，这些路径两两之间是相互依赖的。**前向网络层则没有这些依赖性**，但这些路径在流经前向网络时可以并行执行。\n",
    "\n",
    "Self-Attention中使用多头机制，使得不同的attention heads所关注的的部分不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdnimg.cn/20200714211153687.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编码\"it\"时，一个attention head集中于\"the animal\"，另一个head集中于“tired”，某种意义上讲，模型对“it”的表达合成了的“animal”和“tired”两者。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于自注意力的详细计算，欢迎大家参考[Jay Alammar](https://jalammar.github.io/illustrated-transformer/)关于Transformer的博客，这里不再展开。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除此之外，为了使模型保持单词的语序，模型中添加了位置编码向量。如下图所示，每行对应一个向量的位置编码。因此，第一行将是我们要添加到输入序列中第一个单词的嵌入的向量。每行包含512个值—每个值都在1到-1之间。因为左侧是用sine函数生成，右侧是用cosine生成，所以可以观察到中间显著的分隔。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdnimg.cn/20200714211232716.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编码器结构中值得提出注意的一个细节是，在每个子层中（Self-attention, FFNN），都有残差连接，并且紧跟着[layer-normalization](https://arxiv.org/abs/1607.06450)。如果我们可视化向量和LayerNorm操作，将如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdnimg.cn/20200714211955713.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于预训练语言模型的词表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于预训练语言模型的词表示由于可以建模上下文信息，进而解决传统静态词向量不能建模“一词多义”语言现象的问题。最早提出的ELMo基于两个单向LSTM，将从左到右和从右到左两个方向的隐藏层向量表示拼接学习上下文词嵌入。而GPT用Transformer代替LSTM作为编码器，首先进行了语言模型预训练，然后在下游任务微调模型参数。但GPT由于仅使用了单向语言模型，因此难以建模上下文信息。为了解决以上问题，研究者们提出了BERT，BERT模型结构如下图所示，它是一个基于Transformer的多层Encoder，通过执行一系列预训练，进而得到深层的上下文表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bert_elmo](https://img-blog.csdnimg.cn/20200714211316167.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMo论文题目中Deep是指双向双层LSTM，而更关键的在于context。传统方法生成的单词映射表的形式，即先为每个单词生成一个静态的词向量，之后这个单词的表示就被固定住了，不会跟着上下文的变化而做出改变。事实上，由于一词多义的语言现象，静态词向量是有很大的弊端的。以bank为例，如果训练语料的足够大，事先学好的词向量中混杂着所有的语义。而当下游应用时，即使在新句子中，bank的上下文里包含money等词，我们基本可以确定bank是“银行”的语义而不是在其他上下文中的“河床”的语义，但是由于静态词向量不能跟随上下文而进行变化，所以bank的表示中还是混杂着多种语义。为了解决这一问题，ELMo首先进行了语言模型预训练，然后在下游任务中动态调整Word Embedding，因此最后输出的词表示能够充分表达单词在上下文中的特定语义，进而解决一词多义的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT来自于openai，是一种生成式预训练模型。GPT 除了将ELMo中的LSTM替换为Transformer 的Encoder外，更开创了NLP界基于预训练-微调的新范式。尽管GPT采用的也是和ELMo相同的两阶段模式，但GPT在第一个阶段并没有采取ELMo中使用两个单向双层LSTM拼接的结构，而是采用基于自回归式的单向语言模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google在NAACL 2018发表的论文中提出了BERT，与GPT相同，BERT也采用了预训练-微调这一两阶段模式。但在模型结构方面，BERT采用了ELMO的范式，即使用双向语言模型代替GPT中的单向语言模型，但是BERT的作者认为ELMo使用两个单向语言模型拼接的方式太粗暴，因此在第一阶段的预训练过程中，BERT提出掩码语言模型，即类似完形填空的方式，通过上下文来预测单词本身，而不是从右到左或从左到右建模，这允许模型能够自由地编码每个层中来自两个方向的信息。而为了学习句子的词序关系，BERT将Transformer中的三角函数位置表示替换为可学习的参数，其次为了区别单句和双句输入，BERT还引入了句子类型表征。BERT的输入如图所示。此外，为了充分学习句子间的关系，BERT提出了下一个句子预测任务。具体来说，在训练时，句子对中的第二个句子有50％来自与原有的连续句子，而其余50%的句子则是通过在其他句子中随机采样。同时，消融实验也证明，这一预训练任务对句间关系判断任务具有很大的贡献。除了模型结构不同之外，BERT在预训练时使用的无标签数据规模要比GPT大的多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bert_input](https://img-blog.csdnimg.cn/20200714211348456.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第二阶段，与GPT相同，BERT也使用Fine-Tuning模式来微调下游任务。如下图所示，BERT与GPT不同，它极大的减少了改造下游任务的要求，只需在BERT模型的基础上，通过额外添加Linear分类器，就可以完成下游任务。具体来说，对于句间关系判断任务，与GPT类似，只需在句子之间加个分隔符，然后在两端分别加上起始和终止符号。在进行输出时，只需把句子的起始符号[CLS]在BERT最后一层中对应的位置接一个Softmax+Linear分类层即可；对于单句分类问题，也与GPT类似，只需要在句子两段分别增加起始和终止符号，输出部分和句间关系判断任务保持一致即可；对于问答任务，由于需要输出答案在给定段落的起始和终止位置，因此需要先将问题和段落按照句间关系判断任务构造输入，输出只需要在BERT最后一层中第二个句子，即段落的每个单词对应的位置上分别接判断起始和终止位置的分类器；最后，对于NLP中的序列标注问题，输入与单句分类任务一致，不同的是在BERT最后一层中每个单词对应的位置上接分类器即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bert_task](https://img-blog.csdnimg.cn/20200714211409582.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更重要的是，BERT开启了NLP领域“预训练-微调”这种两阶段的全新范式。在第一阶段首先在海量无标注文本上预训练一个双向语言模型，这里特别值得注意的是，将Transformer作为特征提取器在解决并行性和长距离依赖问题上都要领先于传统的RNN或者CNN，通过预训练的方式，可以将训练数据中的词法、句法、语法知识以网络参数的形式提炼到模型当中，在第二阶段使用下游任务的数据Fine-tuning不同层数的BERT模型参数，或者把BERT当作特征提取器生成BERT Embedding，作为新特征引入下游任务。这种两阶段的全新范式尽管是来自于计算机视觉领域，但是在自然语言处理领域一直没有得到很好的运用，而BERT作为近些年NLP突破性进展的集大成者，最大的亮点可以说不仅在于模型性能好，并且几乎所有NLP任务都可以很方便地基于BERT进行改造，进而将预训练学到的语言学知识引入下游任务，进一步提升模型的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于Bert的文本分类\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert Pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预训练过程使用了Google基于Tensorflow发布的BERT源代码。首先从原始文本中创建训练数据，由于本次比赛的数据都是ID，这里重新建立了词表，并且建立了基于空格的分词器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhitespaceTokenizer(object):\n",
    "    \"\"\"WhitespaceTokenizer with vocab.\"\"\"\n",
    "    def __init__(self, vocab_file):\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        split_tokens = whitespace_tokenize(text)\n",
    "        output_tokens = []\n",
    "        for token in split_tokens:\n",
    "            if token in self.vocab:\n",
    "                output_tokens.append(token)\n",
    "            else:\n",
    "                output_tokens.append(\"[UNK]\")\n",
    "        return output_tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return convert_by_vocab(self.vocab, tokens)\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return convert_by_vocab(self.inv_vocab, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预训练由于去除了NSP预训练任务，因此将文档处理多个最大长度为256的段，如果最后一个段的长度小于256/2则丢弃。每一个段执行按照BERT原文中执行掩码语言模型，然后处理成tfrecord格式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segments_from_document(document, max_segment_length):\n",
    "    \"\"\"Split single document to segments according to max_segment_length.\"\"\"\n",
    "    assert len(document) == 1\n",
    "    document = document[0]\n",
    "    document_len = len(document)\n",
    "\n",
    "    index = list(range(0, document_len, max_segment_length))\n",
    "    other_len = document_len % max_segment_length\n",
    "    if other_len > max_segment_length / 2:\n",
    "        index.append(document_len)\n",
    "\n",
    "    segments = []\n",
    "    for i in range(len(index) - 1):\n",
    "        segment = document[index[i]: index[i+1]]\n",
    "        segments.append(segment)\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在预训练过程中，也只执行掩码语言模型任务，因此不再计算下一句预测任务的loss。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(masked_lm_loss, masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(\n",
    "    bert_config, model.get_sequence_output(), model.get_embedding_table(),\n",
    "    masked_lm_positions, masked_lm_ids, masked_lm_weights)\n",
    "\n",
    "total_loss = masked_lm_loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了适配句子的长度，以及减小模型的训练时间，我们采取了BERT-mini模型，详细配置如下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "  \"hidden_size\": 256,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"vocab_size\": 5981,\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"num_attention_heads\": 4,\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"max_position_embeddings\": 256,\n",
    "  \"num_hidden_layers\": 4,\n",
    "  \"intermediate_size\": 1024,\n",
    "  \"attention_probs_dropout_prob\": 0.1\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于我们的整体框架使用Pytorch，因此需要将最后一个检查点转换成Pytorch的权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, bert_config_file, pytorch_dump_path):\n",
    "    # Initialise PyTorch model\n",
    "    config = BertConfig.from_json_file(bert_config_file)\n",
    "    print(\"Building PyTorch model from configuration: {}\".format(str(config)))\n",
    "    model = BertForPreTraining(config)\n",
    "\n",
    "    # Load weights from tf checkpoint\n",
    "    load_tf_weights_in_bert(model, config, tf_checkpoint_path)\n",
    "\n",
    "    # Save pytorch-model\n",
    "    print(\"Save PyTorch model to {}\".format(pytorch_dump_path))\n",
    "    torch.save(model.state_dict(), pytorch_dump_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预训练消耗的资源较大，硬件条件不允许的情况下建议**直接下载开源的模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert Finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![experimental) Dynamic Quantization on BERT — PyTorch Tutorials 1.5 ...](https://img-blog.csdnimg.cn/20200714211526326.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "微调将最后一层的第一个token即[CLS]的隐藏向量作为句子的表示，然后输入到softmax层进行分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sequence_output, pooled_output = \\\n",
    "    self.bert(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "\n",
    "if self.pooled:\n",
    "    reps = pooled_output\n",
    "else:\n",
    "    reps = sequence_output[:, 0, :]  # sen_num x 256\n",
    "\n",
    "if self.training:\n",
    "    reps = self.dropout(reps)\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本章小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章介绍了Bert的原理和使用，具体包括pretrain和finetune两部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本章作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 完成Bert Pretrain和Finetune的过程\n",
    "- 阅读Bert官方文档，找到相关参数进行调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
