{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1 + Task2 + Task3 +Task4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of this competition: \n",
    "\n",
    "- predict the category of each news. \n",
    "\n",
    "\n",
    "Evaluation: \n",
    "\n",
    "- Avg value of f1_score. \n",
    "\n",
    "- A comparation between submitting category result and real category data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12281897.0.0.7a212b4amWToWd&postId=95703\n",
    "\n",
    "Task1 赛题理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.9.6406111aIKCSLV&postId=118253\n",
    "\n",
    "Task2 数据读取与数据分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12281897.0.0.7a212b4amWToWd&postId=118254\n",
    "\n",
    "https://github.com/datawhalechina/team-learning-nlp/blob/master/NewsTextClassification/Task3%20%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.md\n",
    "\n",
    "Task3 基于机器学习的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12281897.0.0.7a212b4amWToWd&postId=118255\n",
    "\n",
    "Task4 基于深度学习的文本分类1-fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tianchi.aliyun.com/notebook-ai/?spm=5176.12282024.0.0.432d14ca2Inpak\n",
    "\n",
    "https://github.com/datawhalechina/team-learning-nlp/tree/master/NewsTextClassification\n",
    "    \n",
    "All Task1-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://shimo.im/docs/rp3OVnXzDWtpoxAm/read\n",
    "    \n",
    "石墨文档大纲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**听课笔记**\n",
    "- Part1 比赛介绍\n",
    "- Part2 baseline\n",
    "- Part3 比赛知识点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个进行文本分类的数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么匿名数据集？\n",
    "- 没有版权问题；\n",
    "- 需要从头构建词向量。（不能分词）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思路：（四种方法，由易到难）\n",
    "- TF-IDF + RidgeClassifier ; (F1_score ~ 0.93)\n",
    "- FastText ; (F1_score ~ 0.93-0.94)\n",
    "- Word2Vec + TextCNN ; (F1_score ~ 0.95-0.97)\n",
    "- Bert.(F1_score ~ 0.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思路1 \n",
    "1 ：将字符进行 TF-IDF (term frequency–inverse document frequency）统计，然后送入线性分类器进行训练；\n",
    "- CountVectorizer RidgeClassifier\n",
    "- TfidfVectorizer RidgeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。tf-idf加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了tf-idf以外，互联网上的搜索引擎还会使用基于链接分析的评级方法，以确定文件在搜索结果中出现的顺序。)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline\n",
    "- 如何继续深入，提高精度？\n",
    "- 尝试其他机器学习模型；\n",
    "- 对 TF IDF 和 ngram 进行 gridsearch\n",
    "- 尝试思路 2 、思路 3 、思路 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其它尝试和思考：\n",
    "你能分析得出匿名字符中的标点符号吗？ （可以出现结尾，但不开头）\n",
    "你知道 NLP 中哪些数据扩增方法呢？（同义词替换，添加某个单词，等等）\n",
    "线上 F1 打分能到 0.99 吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding and exploratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testa=pd.read_csv('C:/Chen/1_Coding/20200721_nlp_learning/test_a.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('C:/Chen/1_Coding/20200721_nlp_learning/train_set.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5399 3117 1070 4321 4568 2621 5466 3772 4516 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2491 4109 1757 7539 648 3695 3038 4490 23 7019...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2673 5076 6835 2835 5948 5677 3247 4124 2465 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4562 4893 2210 4761 3659 1324 2595 5949 4583 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4269 7134 2614 1724 4464 1324 3370 3370 2106 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>3725 4498 2282 1647 6293 4245 4498 3615 1141 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>4811 465 3800 1394 3038 2376 2327 5165 3070 57...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>5338 1952 3117 4109 299 6656 6654 3792 6831 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>893 3469 5775 584 2490 4223 6569 6663 2124 168...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>2400 4409 4412 2210 5122 4464 7186 2465 1327 9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      5399 3117 1070 4321 4568 2621 5466 3772 4516 2...\n",
       "1      2491 4109 1757 7539 648 3695 3038 4490 23 7019...\n",
       "2      2673 5076 6835 2835 5948 5677 3247 4124 2465 5...\n",
       "3      4562 4893 2210 4761 3659 1324 2595 5949 4583 2...\n",
       "4      4269 7134 2614 1724 4464 1324 3370 3370 2106 2...\n",
       "49995  3725 4498 2282 1647 6293 4245 4498 3615 1141 2...\n",
       "49996  4811 465 3800 1394 3038 2376 2327 5165 3070 57...\n",
       "49997  5338 1952 3117 4109 299 6656 6654 3792 6831 21...\n",
       "49998  893 3469 5775 584 2490 4223 6569 6663 2124 168...\n",
       "49999  2400 4409 4412 2210 5122 4464 7186 2465 1327 9..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testa.head().append(df_testa.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2967 6758 339 2021 1854 3731 4109 3792 4149 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>4464 486 6352 5619 2465 4802 1452 3137 5778 54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7346 4068 5074 3747 5681 6093 1777 2226 7354 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>7159 948 4866 2109 5520 2490 211 3956 5520 549...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3646 3055 3055 2490 4659 6065 3370 5814 2465 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>2</td>\n",
       "      <td>307 4894 7539 4853 5330 648 6038 4409 3764 603...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>2</td>\n",
       "      <td>3792 2983 355 1070 4464 5050 6298 3782 3130 68...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>11</td>\n",
       "      <td>6811 1580 7539 1252 1899 5139 1386 3870 4124 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>2</td>\n",
       "      <td>6405 3203 6644 983 794 1913 1678 5736 1397 191...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>3</td>\n",
       "      <td>4350 3878 3268 1699 6909 5505 2376 2465 6088 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                               text\n",
       "0           2  2967 6758 339 2021 1854 3731 4109 3792 4149 15...\n",
       "1          11  4464 486 6352 5619 2465 4802 1452 3137 5778 54...\n",
       "2           3  7346 4068 5074 3747 5681 6093 1777 2226 7354 6...\n",
       "3           2  7159 948 4866 2109 5520 2490 211 3956 5520 549...\n",
       "4           3  3646 3055 3055 2490 4659 6065 3370 5814 2465 5...\n",
       "199995      2  307 4894 7539 4853 5330 648 6038 4409 3764 603...\n",
       "199996      2  3792 2983 355 1070 4464 5050 6298 3782 3130 68...\n",
       "199997     11  6811 1580 7539 1252 1899 5139 1386 3870 4124 1...\n",
       "199998      2  6405 3203 6644 983 794 1913 1678 5736 1397 191...\n",
       "199999      3  4350 3878 3268 1699 6909 5505 2376 2465 6088 2..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head().append(df_train.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 2 columns):\n",
      "label    200000 non-null int64\n",
      "text     200000 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 1 columns):\n",
      "text    50000 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 390.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_testa.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.210950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.084955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label\n",
       "count  200000.000000\n",
       "mean        3.210950\n",
       "std         3.084955\n",
       "min         0.000000\n",
       "25%         1.000000\n",
       "50%         2.000000\n",
       "75%         5.000000\n",
       "max        13.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2538 2506 1363 5466 3772 340 922 433 2397 5778...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "count                                               50000\n",
       "unique                                              49995\n",
       "top     2538 2506 1363 5466 3772 340 922 433 2397 5778...\n",
       "freq                                                    4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testa.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "count    200000.000000\n",
      "mean        907.207110\n",
      "std         996.029036\n",
      "min           2.000000\n",
      "25%         374.000000\n",
      "50%         676.000000\n",
      "75%        1131.000000\n",
      "max       57921.000000\n",
      "Name: text_len, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#In order to know the length of sentences\n",
    "#The number of words in sentence\n",
    "%pylab inline\n",
    "df_train['text_len'] = df_train['text'].apply(lambda x: len(x.split(' ')))\n",
    "print(df_train['text_len'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In resume, this competition gives relatively long sentences. A sentence is composed by 907 words in average, with the minimum length of 2, and the maximum length of 44665 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viz of sentences' length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram of char count')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Viz of sentences' length\n",
    "_ = plt.hist(df_train['text_len'], bins=200)\n",
    "plt.xlabel('Text char count')\n",
    "plt.title(\"Histogram of char count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viz of news category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'category')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Viz of news category\n",
    "df_train['label'].value_counts().plot(kind='bar')\n",
    "plt.title('News class count')\n",
    "plt.xlabel(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在数据集中标签的对应的关系如下：{'科技': 0, '股票': 1, '体育': 2, '娱乐': 3, '时政': 4, '社会': 5, '教育': 6, '财经': 7, '家居': 8, '游戏': 9, '房产': 10, '时尚': 11, '彩票': 12, '星座': 13}\n",
    "从统计结果可以看出，赛题的数据集类别分布存在较为不均匀的情况。在训练集中科技类新闻最多，其次是股票类新闻，最少的新闻是星座新闻。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequencty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the most/least frequent word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lines = ' '.join(list(df_train['text']))\n",
    "type(all_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter(all_lines.split(\" \"))\n",
    "word_count = sorted(word_count.items(), key=lambda d:int(d[1]), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6869\n"
     ]
    }
   ],
   "source": [
    "print(len(word_count))\n",
    "# in total there are 6869 words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('3750', 7482224)\n",
      "('648', 4924890)\n",
      "('900', 3262544)\n",
      "('3370', 2020958)\n",
      "('6122', 1602363)\n"
     ]
    }
   ],
   "source": [
    "print(word_count[0])\n",
    "print(word_count[1])\n",
    "print(word_count[2])\n",
    "print(word_count[3])\n",
    "print(word_count[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('3133', 1)\n"
     ]
    }
   ],
   "source": [
    "print(word_count[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find punctuation marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里还可以根据字在每个句子的出现情况，反推出标点符号。下面代码统计了不同字符在句子中出现的次数，其中字符3750，字符900和字符648在20w新闻的覆盖率接近99%，很有可能是标点符号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text_unique'] = df_train['text'].apply(lambda x: ' ' .join(list(set(x.split(' ')))))\n",
    "#df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines1 = ' '.join(list(df_train['text_unique']))\n",
    "word_count1 = Counter(all_lines1.split(\" \"))\n",
    "word_count1 = sorted(word_count1.items(), key=lambda d:int(d[1]), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('3750', 197997)\n",
      "('900', 197653)\n",
      "('648', 191975)\n",
      "('2465', 177310)\n",
      "('6122', 176543)\n"
     ]
    }
   ],
   "source": [
    "print(word_count1[0])\n",
    "print(word_count1[1])\n",
    "print(word_count1[2])\n",
    "print(word_count1[3])\n",
    "print(word_count1[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据分析的结论\n",
    "通过上述分析我们可以得出以下结论：\n",
    "\n",
    "赛题中每个新闻包含的字符个数平均为1000个，还有一些新闻字符较长；\n",
    "赛题中新闻类别分布不均匀，科技类新闻样本量接近4w，星座类新闻样本量不到1k；\n",
    "赛题总共包括7000-8000个字符；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过数据分析，我们还可以得出以下结论：\n",
    "\n",
    "每个新闻平均字符个数较多，可能需要截断；\n",
    "\n",
    "由于类别不均衡，会严重影响模型的精度；\n",
    "\n",
    "本章小结\n",
    "本章对赛题数据进行读取，并新闻句子长度、类别和字符进行了可视化分析。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章作业\n",
    "假设字符3750，字符900和字符648是句子的标点符号，请分析赛题每篇新闻平均由多少个句子构成？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_count[0][0])\n",
    "#word_count[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5298"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_number = int(word_count[0][0]) + int(word_count[1][0]) + int(word_count[2][0])\n",
    "sentence_number "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计每类新闻中出现次数对多的字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0     18587\n",
       "1     57921\n",
       "2     41894\n",
       "3     10817\n",
       "4     14928\n",
       "5     15839\n",
       "6     25728\n",
       "7     14469\n",
       "8     15271\n",
       "9     23866\n",
       "10    20622\n",
       "11     5729\n",
       "12     8737\n",
       "13     6399\n",
       "Name: text_len, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df_train.groupby(['label'], sort=True)['text_len'].max()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task3 基于机器学习的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一章节，我们对赛题的数据进行了读取，并在末尾给出了两个小作业。如果你顺利完成了作业，那么你基本上对`Python`也比较熟悉了。在本章我们将使用传统机器学习算法来完成新闻分类的过程，将会结束到赛题的核心知识点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于机器学习的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本章我们将开始使用机器学习模型来解决文本分类。机器学习发展比较广，且包括多个分支，本章侧重使用传统机器学习，从下一章开始是基于深度学习的文本分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 学会TF-IDF的原理和使用\n",
    "- 使用sklearn的机器学习模型完成文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 机器学习模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "机器学习是对能通过经验自动改进的计算机算法的研究。机器学习通过历史数据**训练**出**模型**对应于人类对经验进行**归纳**的过程，机器学习利用**模型**对新数据进行**预测**对应于人类利用总结的**规律**对新问题进行**预测**的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习有很多种分支，对于学习者来说应该优先掌握机器学习算法的分类，然后再其中一种机器学习算法进行学习。由于机器学习算法的分支和细节实在是太多，所以如果你一开始就被细节迷住了眼，你就很难知道全局是什么情况的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你是机器学习初学者，你应该知道如下的事情：\n",
    "\n",
    "1. 机器学习能解决一定的问题，但不能奢求机器学习是万能的；\n",
    "2. 机器学习算法有很多种，看具体问题需要什么，再来进行选择；\n",
    "3. 每种机器学习算法有一定的偏好，需要具体问题具体分析；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![machine_learning_overview](https://img-blog.csdnimg.cn/20200714203223253.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本表示方法 Part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在机器学习算法的训练过程中，假设给定$N$个样本，每个样本有$M$个特征，这样组成了$N×M$的样本矩阵，然后完成算法的训练和预测。同样的在计算机视觉中可以将图片的像素看作特征，每张图片看作hight×width×3的特征图，一个三维的矩阵来进入计算机进行计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是在自然语言领域，上述方法却不可行：文本是不定长度的。文本表示成计算机能够运算的数字或向量的方法一般称为词嵌入（Word Embedding）方法。词嵌入将不定长的文本转换到定长的空间内，是文本分类的第一步。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的One-hot与数据挖掘任务中的操作是一致的，即将每一个单词使用一个离散的向量表示。具体将每个字/词编码一个索引，然后根据索引进行赋值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot表示方法的例子如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "句子1：我 爱 北 京 天 安 门\n",
    "句子2：我 喜 欢 上 海\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先对所有句子的字进行索引，即将每个字确定一个编号："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "{\n",
    "\t'我': 1, '爱': 2, '北': 3, '京': 4, '天': 5,\n",
    "  '安': 6, '门': 7, '喜': 8, '欢': 9, '上': 10, '海': 11\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里共包括11个字，因此每个字可以转换为一个11维度稀疏向量："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "我：[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "爱：[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "...\n",
    "海：[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words（词袋表示），也称为Count Vectors，每个文档的字/词可以使用其出现次数来进行表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "句子1：我 爱 北 京 天 安 门\n",
    "句子2：我 喜 欢 上 海\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接统计每个字出现的次数，并进行赋值："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "句子1：我 爱 北 京 天 安 门\n",
    "转换为 [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "\n",
    "句子2：我 喜 欢 上 海\n",
    "转换为 [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在sklearn中可以直接`CountVectorizer`来实现这一步骤："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer() \n",
    "vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "# CountVectorizer() : Get the term frequency\n",
    "# n_gram好像是用来组词的，几个词一组？\n",
    "#大小写不敏感\n",
    "#出现的英文词，按alphabet重排序了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram与Count Vectors类似，不过加入了相邻单词组合成为新的单词，并进行计数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果N取值为2，则句子1和句子2就变为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "句子1：我爱 爱北 北京 京天 天安 安门\n",
    "句子2：我喜 喜欢 欢上 上海\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF 分数由两部分组成：第一部分是**词语频率**（Term Frequency），第二部分是**逆文档频率**（Inverse Document Frequency）。其中计算语料库中文档总数除以含有该词语的文档数量，然后再取对数就是逆文档频率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TF(t)= 该词语在当前文档出现的次数 / 当前文档中词语的总数\n",
    "IDF(t)= log_e（文档总数 / 出现该词语的文档总数）\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于机器学习的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们将对比不同文本表示算法的精度，通过本地构建验证集计算F1得分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectors + RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import f1_score \n",
    "\n",
    "\n",
    "df_train=pd.read_csv('C:/Chen/1_Coding/20200721_nlp_learning/train_set.csv',sep='\\t', nrows=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-22ff0580af54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRidgeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mval_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    921\u001b[0m                              compute_sample_weight(self.class_weight, y))\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 923\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    924\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    563\u001b[0m                 \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m                 \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_n_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m                 return_intercept=True, check_input=False)\n\u001b[0m\u001b[0;32m    566\u001b[0m             \u001b[1;31m# add the offset which was subtracted by _preprocess_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0my_offset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py\u001b[0m in \u001b[0;36m_ridge_regression\u001b[1;34m(X, y, alpha, sample_weight, solver, max_iter, tol, verbose, random_state, return_n_iter, return_intercept, X_scale, X_offset, check_input)\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m                 \u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m                 is_saga=solver == 'saga')\n\u001b[0m\u001b[0;32m    493\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreturn_intercept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m                 \u001b[0mcoef\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py\u001b[0m in \u001b[0;36msag_solver\u001b[1;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[0;32m    331\u001b[0m                             \u001b[0mintercept_decay\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m                             \u001b[0mis_saga\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m                             verbose)\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=3000)\n",
    "train_test = vectorizer.fit_transform(df_train['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:10000], df_train['label'].values[:10000])\n",
    "\n",
    "val_pred = clf.predict(train_test[10000:])\n",
    "print(f1_score(df_train['label'].values[10000:], val_pred, average='macro'))\n",
    "#0.654430246247168"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  TF-IDF +  RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "df_train=pd.read_csv('C:/Chen/1_Coding/20200721_nlp_learning/train_set.csv',sep='\\t', nrows=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=3000)\n",
    "train_test = tfidf.fit_transform(df_train['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:10000], df_train['label'].values[:10000])\n",
    "\n",
    "val_pred = clf.predict(train_test[10000:])\n",
    "print(f1_score(df_train['label'].values[10000:], val_pred, average='macro'))\n",
    "\n",
    "# CountVectorizer() : Get the term frequency\n",
    "# TfidfVectorizer() : based on CountVectorizer() \n",
    "# 需要先用CountVectorizer()\n",
    "#0.8719372173702"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**以下是一个来自Sklearn官方文档的例子**\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "\n",
    "另外还有两个相关文档：\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "corpus = ['this is the first document',\n",
    "          'this document is the second document',\n",
    "          'and this is the third one',\n",
    "          'is this the first document']\n",
    "vocabulary = ['this', 'document', 'first', 'is', 'second', 'the', 'and', 'one']\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)), \n",
    "                 # CountVectorizer() : 计算文档中所有包括该词的数目\n",
    "                 ('tfid', TfidfTransformer())]).fit(corpus)\n",
    "pipe['count'].transform(corpus).toarray()\n",
    "\"\"\"\n",
    "array([[1, 1, 1, 1, 0, 1, 0, 0],\n",
    "       [1, 2, 0, 1, 1, 1, 0, 0],\n",
    "       [1, 0, 0, 1, 0, 1, 1, 1],\n",
    "       [1, 1, 1, 1, 0, 1, 0, 0]])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe['tfid'].idf_\n",
    "\"\"\"\n",
    "array([1.        , 1.22314355, 1.51082562, 1.        , 1.91629073,\n",
    "       1.        , 1.91629073, 1.91629073])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.transform(corpus).shape\n",
    "\n",
    "\"\"\"\n",
    "(4, 8)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以《中国的蜜蜂养殖》为例，假定该文长度为1000个词，\"中国\"、\"蜜蜂\"、\"养殖\"各出现20次，则这三个词的\"词频\"（TF）都为0.02。然后，搜索Google发现，包含\"的\"字的网页共有250亿张，假定这就是中文网页总数。包含\"中国\"的网页共有62.3亿张，包含\"蜜蜂\"的网页为0.484亿张，包含\"养殖\"的网页为0.973亿张。则它们的逆文档频率（IDF）和TF-IDF如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tfidf(wtermfreq, alltermfreq, wdocfreq, alldocfreq):\n",
    "    tf = round(wtermfreq/alltermfreq,4)\n",
    "    idf = round(math.log10(wdocfreq/(alldocfreq+1)),4)\n",
    "    tfidf = round(tf*idf,4) \n",
    "    print (\"tf=\"+str(tf)+\",\",\"idf=\"+str(idf)+\",\",\"tfidf=\"+str(tfidf))\n",
    "\n",
    "#中国\n",
    "tfidf(20,1000,250*10**8,62.3*10**8)\n",
    "\n",
    "#蜜蜂\n",
    "tfidf(20,1000,250*10**8,0.484*10**8)\n",
    "\n",
    "#养殖\n",
    "tfidf(20,1000,250*10**8,0.973*10**8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上表可见，\"蜜蜂\"的TF-IDF值最高，\"养殖\"其次，\"中国\"最低。（如果还计算\"的\"字的TF-IDF，那将是一个极其接近0的值。）所以，如果只选择一个词，\"蜜蜂\"就是这篇文章的关键词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了自动提取关键词，TF-IDF算法还可以用于许多别的地方。比如，信息检索时，对于每个文档，都可以分别计算一组搜索词（\"中国\"、\"蜜蜂\"、\"养殖\"）的TF-IDF，将它们相加，就可以得到整个文档的TF-IDF。这个值最高的文档就是与搜索词最相关的文档。\n",
    "TF-IDF算法的优点是简单快速，结果比较符合实际情况。缺点是，单纯以\"词频\"衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。（一种解决方法是，对全文的第一段和每一段的第一句话，给予较大的权重。）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本章小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章我们介绍了基于机器学习的文本分类方法，并完成了两种方法的对比。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本章作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 尝试改变TF-IDF的参数，并验证精度\n",
    "2. 尝试使用其他机器学习模型，完成训练和验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 尝试改变TF-IDF的参数，并验证精度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "看paramater部分，调参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1. \n",
    "TF-IDF + RidgeClassifier \n",
    "把总数据量调大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "df_train1=pd.read_csv('C:/Chen/1_Coding/20200721_nlp_learning/train_set.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=3000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. \n",
    "TF-IDF + RidgeClassifier \n",
    "在1.1的基础上把max_feature调大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. \n",
    "TF-IDF + RidgeClassifier \n",
    "在1.1的基础上把ngram_range和max_feature调大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,4), max_features=4000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.尝试使用其他机器学习模型，完成训练和验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类器总结\n",
    "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + LR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + svm.linearSVR\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = LinearSVR()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + svm.SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + knn_classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + DT_classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + GBDT_classifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = GradientBoostingRegressor()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + RF_classifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = RandomForestRegressor()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + Xgboost_classifier\n",
    "from sklearn.tree import RandomForestRegressor\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = RandomForestRegressor()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=30000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "clf = MLPClassifier()\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "\n",
    "val_pred = clf.predict(train_test[:])\n",
    "print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf + svm.linearSVR(gridsearch)\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=20000)\n",
    "train_test = tfidf.fit_transform(df_train1['text'])\n",
    "\n",
    "parameters = {\"gamma\":[0.001,0.01,0.1,1,10,100], \"C\":[0.001,0.01,0.1,1,10,100]}\n",
    "clf = LinearSVR(gamma, C, cv=5)\n",
    "clf.fit(train_test[:], df_train1['label'].values[:])\n",
    "clf.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LinearSVR(gamma=x, C=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_pred = clf.predict(train_test[:])\n",
    "#print(f1_score(df_train1['label'].values[:], val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task4 基于深度学习的文本分类1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一章节，我们使用传统机器学习算法来解决了文本分类问题，从本章开始我们将尝试使用深度学习方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于深度学习的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与传统机器学习不同，深度学习既提供特征提取功能，也可以完成分类的功能。从本章开始我们将学习如何使用深度学习来完成文本表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 学习FastText的使用和基础原理\n",
    "- 学会使用验证集进行调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本表示方法 Part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 现有文本表示方法的缺陷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一章节，我们介绍几种文本表示方法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One-hot\n",
    "- Bag of Words\n",
    "- N-gram\n",
    "- TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也通过sklean进行了相应的实践，相信你也有了初步的认知。但上述方法都或多或少存在一定的问题：转换得到的向量维度很高，需要较长的训练实践；没有考虑单词与单词之间的关系，只是进行了统计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与这些表示方法不同，深度学习也可以用于文本表示，还可以将其映射到一个低纬空间。其中比较典型的例子有：FastText、Word2Vec和Bert。在本章我们将介绍FastText，将在后面的内容介绍Word2Vec和Bert。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText是一种典型的深度学习词向量的表示方法，它非常简单通过Embedding层将单词映射到稠密空间，然后将句子中所有的单词在Embedding空间中进行平均，进而完成分类操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以FastText是一个三层的神经网络，输入层、隐含层和输出层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fast_text](https://img-blog.csdnimg.cn/20200714204856589.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图是使用keras实现的FastText网络结构："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![keras_fasttext](https://img-blog.csdnimg.cn/20200714204249463.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText在文本分类任务上，是优于TF-IDF的："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FastText用单词的Embedding叠加获得的文档向量，将相似的句子分为一类\n",
    "- FastText学习到的Embedding空间维度比较低，可以快速进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想深度学习，可以参考论文："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Tricks for Efficient Text Classification, https://arxiv.org/abs/1607.01759"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于FastText的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText可以快速的在CPU上进行训练，最好的实践方法就是官方开源的版本：\n",
    "https://github.com/facebookresearch/fastText/tree/master/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pip安装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pip install fasttext\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "conda install -c mbednarski fasttext\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 源码安装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "git clone https://github.com/facebookresearch/fastText.git\n",
    "cd fastText\n",
    "sudo pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两种安装方法都可以安装，如果你是初学者可以优先考虑使用pip安装。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 转换为FastText需要的格式\n",
    "df_train = pd.read_csv('C:/Chen/1_Coding/20200721_nlp_learning/train_set.csv', sep='\\t', nrows=15000)\n",
    "df_train['label_ft'] = '__label__' + df_train['label'].astype(str)\n",
    "df_train[['text','label_ft']].iloc[:-5000].to_csv('train.csv', index=None, header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8238894886303253\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised('train.csv', lr=1.0, wordNgrams=2, \n",
    "                                  verbose=2, minCount=1, epoch=25, loss=\"hs\")\n",
    "\n",
    "val_pred = [model.predict(x)[0][0].split('__')[-1] for x in df_train.iloc[-5000:]['text']]\n",
    "print(f1_score(df_train['label'].values[-5000:].astype(str), val_pred, average='macro'))\n",
    "# 0.8238894886303253"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时数据量比较小得分为0.82，当不断增加训练集数量时，FastText的精度也会不断增加5w条训练样本时，验证集得分可以到0.89-0.90左右。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何使用验证集调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用TF-IDF和FastText中，有一些模型的参数需要选择，这些参数会在一定程度上影响模型的精度，那么如何选择这些参数呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过阅读文档，要弄清楚这些参数的大致含义，那些参数会增加模型的复杂度\n",
    "- 通过在验证集上进行验证模型精度，找到模型在是否过拟合还是欠拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![train_val](https://img-blog.csdnimg.cn/20200714204403844.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们使用10折交叉验证，每折使用9/10的数据进行训练，剩余1/10作为验证集检验模型的效果。这里需要注意每折的划分必须保证标签的分布与整个数据集的分布一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "label2id = {}\n",
    "for i in range(total):\n",
    "    label = str(all_labels[i])\n",
    "    if label not in label2id:\n",
    "        label2id[label] = [i]\n",
    "    else:\n",
    "        label2id[label].append(i)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过10折划分，我们一共得到了10份分布一致的数据，索引分别为0到9，每次通过将一份数据作为验证集，剩余数据作为训练集，获得了所有数据的10种分割。不失一般性，我们选择最后一份完成剩余的实验，即索引为9的一份做为验证集，索引为1-8的作为训练集，然后基于验证集的结果调整超参数，使得模型性能更优。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本章小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章介绍了FastText的原理和基础使用，并进行相应的实践。然后介绍了通过10折交叉验证划分数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本章作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 阅读FastText的文档，尝试修改参数，得到更好的分数\n",
    "- 基于验证集的结果调整超参数，使得模型性能更优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文档在这\n",
    "https://pypi.org/project/fasttext/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
